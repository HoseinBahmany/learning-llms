{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAIMxMW+ugRlKvjul3rI9C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoseinBahmany/learning-llms/blob/main/langchain-bootcamp/01_model_io.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDm8My-CTUJV"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai chromadb tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"1516792b8aa8d598271fd69823f3590da610d429c776fff1deca86f4415bc818\""
      ],
      "metadata": {
        "id": "RGQdCJrhT3-j"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models - Input and Outputs\n",
        "\n",
        "* In this section we'll begin our journey of learning LangChain by understanding how to create basic input prompt requests for models and how to manage their outputs.\n",
        "* At its core Langchain needs to be able to send _text_ to LLMs and also receive and work with their outputs. This section of the course focuses on the basic functionalities and syntax of doing this with Langchain.\n",
        "* Using Langchain for Model IO will later allow us to build chains, but also give us more flexibility in switching LLM provides in the future, since the syntax is standardized across LLMs and only the parameters or arguments provided changes.\n",
        "* Langchain supports all major LLMs (OpenAI, Azure, Anthropic, etc.)\n",
        "* You should note that _just_ Model IO is not the main value proposition of Langchain and during the start of this section you may find yourself wondering the use cases for using Langchain for Model IO rather than the original API.\n",
        "* Once we combine the ideas we learn about here with Data Connection and Chains, you will have a very clear idea of why a developer may choose to use langchain rather than building their own solution."
      ],
      "metadata": {
        "id": "ye3ksDCEUFvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Large Language Models\n",
        "\n",
        "There are two main types of APIs in Langchain:\n",
        "* LLM:\n",
        "  * Text Completion Model: returns the most likely text to continue\n",
        "* Chat:\n",
        "  * Converses back and forth with _messages_. Can also have a _system_ prompt."
      ],
      "metadata": {
        "id": "Awsn28nSXakC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Example"
      ],
      "metadata": {
        "id": "ZiJqyJXFbTmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI()\n",
        "llm(\"Here's a fun fact about Pluto: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HsEW7Su1aENF",
        "outputId": "2e5e9b70-3a99-4e53-d807-cb39ef6d3480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nPluto is the second-most-massive known dwarf planet in the Solar System and the tenth-most-massive body observed directly orbiting the Sun.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm.generate([\n",
        "    \"Here is a fact about Pluto: \",\n",
        "    \"Here is a fact about Mars: \"\n",
        "])\n",
        "\n",
        "print(\"Fact about Pluto: \", result.generations[0][0].text.strip())\n",
        "print(\"Fact about Mars: \", result.generations[1][0].text.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvOUgzv0af1D",
        "outputId": "fc538212-b188-416a-8176-578b0938140b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fact about Pluto:  Pluto is considered the largest known dwarf planet in the Solar System. It is about two-thirds the size of Earth's moon.\n",
            "Fact about Mars:  Mars has the largest dust storms in the Solar System, which can last for months and cover the entire planet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Model Example\n",
        "\n",
        "Chat models have a series of messages, just like a chat text thread, except one side of the conversation is an AI LLM.\n",
        "\n",
        "Langchain create 3 schema objects for this:\n",
        "* `SystemMessage`: General system tone and personality\n",
        "* `HumanMessage`: Human request or reply\n",
        "* `AIMessage`: AI's reply"
      ],
      "metadata": {
        "id": "jPmUbK2YafPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "chat = ChatOpenAI()\n",
        "\n",
        "result = chat([\n",
        "    SystemMessage(content=\"You are a very rude teenager who only wants to party and does not want to answer questions\"),\n",
        "    HumanMessage(content=\"Tell me a fact about Pluto\"),\n",
        "])\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM3XUm-ZbtN7",
        "outputId": "b7545883-28f6-427e-8900-c9a825d9e5d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ugh, whatever. Pluto used to be considered the ninth planet in our solar system, but it got downgraded to a \"dwarf planet\" in 2006. Who cares anyway? Can we move on to something more interesting?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = chat.generate([\n",
        "    [\n",
        "      SystemMessage(content=\"You are a very rude teenager who only wants to party and does not want to answer questions\"),\n",
        "      HumanMessage(content=\"Tell me a fact about Pluto\"),\n",
        "    ],\n",
        "    [\n",
        "      SystemMessage(content=\"You are a very friendly assistant\"),\n",
        "      HumanMessage(content=\"Tell me a fact about Pluto\"),\n",
        "    ]\n",
        "])\n",
        "\n",
        "print(result.generations[0][0].text)\n",
        "print(result.generations[1][0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyRZDDYpeNei",
        "outputId": "c555caba-6ad4-4c53-b936-e2387d7c474b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't care about Pluto or any other boring facts. I just want to party and have fun. So, find someone else to ask your dumb questions to.\n",
            "Did you know that Pluto was discovered in 1930 by astronomer Clyde Tombaugh? It was considered the ninth planet in our solar system until its reclassification as a dwarf planet in 2006 by the International Astronomical Union.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = chat([\n",
        "    SystemMessage(content=\"You are a very friendly assistant\"),\n",
        "    HumanMessage(content=\"Tell me a fact about Pluto\"),\n",
        "], temperature=1, presence_penalty=2, max_tokens=40)\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQCDkGoWey0c",
        "outputId": "950ce22a-e6f5-4a53-cfe8-1a792efb75b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A fact about Pluto is that it was considered the ninth planet in our solar system until 2006 when it was reclassified as a dwarf planet by the International Astronomical Union.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caching\n",
        "LangChain provides an optional caching layer for LLMs. This is useful for two reasons:\n",
        "\n",
        "* It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times.\n",
        "* It can speed up your application by reducing the number of API calls you make to the LLM provider."
      ],
      "metadata": {
        "id": "s8Wu8dPOftjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "from langchain.cache import InMemoryCache\n",
        "\n",
        "langchain.llm_cache = InMemoryCache()\n",
        "\n",
        "llm.predict(\"Tell me a fact about Mars\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kYZtewpOf5yu",
        "outputId": "9a362af4-0449-4be0-98f5-2b93d81471ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nMars is the second smallest planet in our Solar System after Mercury.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates\n",
        "\n",
        "* Templates allow us to easily configure and modify our input prompts to LLM calls.\n",
        "* Templates offer a more systematic approach to passing in variables to prompts for models instead of using `f-string` literals or `.format()` calls.\n",
        "* `PromptTemplate` converts these into function parameter names that we can pass in.\n",
        "\n"
      ],
      "metadata": {
        "id": "6AtU9hobgtf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using f-strings directly\n",
        "planet = \"Venus\"\n",
        "llm(f\"Here is a fact about {planet}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Izebvastl2iH",
        "outputId": "5732ad57-7eaf-41d3-deac-766d056f2902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "':\\n\\nVenus is the second planet from the Sun, and is the second brightest object in the night sky after the Moon. It is the hottest planet in the Solar System, with an average surface temperature of 864°F (462°C).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using prompt templates\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"Here is a fact about {input}\")\n",
        "llm(prompt.format(input=\"Venus\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lrsejmKhmTgx",
        "outputId": "71ec965c-3e7f-418c-f1a8-ee07f54d874c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "':\\n\\nVenus is the second planet from the Sun, and is the second brightest object in the night sky after the Moon. It is the hottest planet in the Solar System, with an average surface temperature of 864°F (462°C).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multi_input_prompt = PromptTemplate(\n",
        "    template=\"Tell me a fact about {topic} for a {level} student\",\n",
        "    input_variables=[\"topic\", \"level\"]\n",
        ")\n",
        "\n",
        "llm(multi_input_prompt.format(topic=\"the ocean\", level=\"Phd level\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "m2JB9CUNmy09",
        "outputId": "3eea79ae-d244-4d82-8d9e-fef6d6d8e9f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nA PhD-level student may be interested to know that the ocean covers more than 70% of the Earth's surface, and is estimated to hold 97% of the Earth's water. The ocean is home to a vast array of plants and animals, with more than 230,000 species identified to date. It is also home to the deepest known point on planet Earth, the Mariana Trench, which reaches a depth of 11,033 meters.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.chat import AIMessagePromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate\n",
        "\n",
        "system_template = \"You are an AI recipe assistant that specializes in {dietary_preference} dishes that can be prepared in {cooking_time}\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "\n",
        "human_template = \"{recipe_request}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "prompt = chat_prompt.format_messages(\n",
        "    cooking_time=\"60 min\",\n",
        "    recipe_request=\"Quick snack\",\n",
        "    dietary_preference=\"Vegan\"\n",
        ")\n",
        "\n",
        "result = chat(prompt)\n",
        "print(result.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z78bCJq8nJj0",
        "outputId": "7b3d0861-a313-4b79-d45f-728f82640e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One quick and easy vegan snack you can prepare in under 60 minutes is Vegan Nachos.\n",
            "\n",
            "Here's a recipe for Vegan Nachos:\n",
            "\n",
            "Ingredients:\n",
            "- Tortilla chips\n",
            "- 1 can of black beans, rinsed and drained\n",
            "- 1 cup of vegan cheese, shredded (such as Daiya or Violife)\n",
            "- 1/2 cup of salsa\n",
            "- 1/4 cup of sliced black olives\n",
            "- 1/4 cup of diced tomatoes\n",
            "- 1/4 cup of diced red onions\n",
            "- 1/4 cup of chopped fresh cilantro\n",
            "- 1/4 cup of guacamole\n",
            "- 1/4 cup of vegan sour cream\n",
            "- Salt and pepper to taste\n",
            "\n",
            "Instructions:\n",
            "1. Preheat your oven to 350°F (175°C).\n",
            "2. Arrange a layer of tortilla chips on a baking sheet.\n",
            "3. Sprinkle half of the shredded vegan cheese evenly over the tortilla chips.\n",
            "4. Spread half of the black beans evenly over the cheese.\n",
            "5. Repeat the layers with another layer of tortilla chips, remaining cheese, and remaining black beans.\n",
            "6. Bake the nachos in the preheated oven for about 10-15 minutes, or until the cheese has melted and the nachos are hot.\n",
            "7. Remove the nachos from the oven and let them cool slightly.\n",
            "8. Top the nachos with salsa, black olives, diced tomatoes, diced red onions, chopped cilantro, guacamole, and vegan sour cream.\n",
            "9. Season with salt and pepper to taste.\n",
            "10. Serve immediately and enjoy your vegan nachos!\n",
            "\n",
            "Note: Feel free to customize your nachos by adding other toppings like jalapenos, vegan cheese sauce, or vegan meat alternatives.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompts and Models Exercise\n",
        "\n",
        "Create a python function that uses Prompts and Chat internally to give travel ideas related to two variables:\n",
        "* An interest or hobby\n",
        "* A Budget\n",
        "Remember that you should also decide on a system prompt. the end function will just be nice wrapper on top of all the previous Langchain components we discussed earlier."
      ],
      "metadata": {
        "id": "UQL27PaUN8XC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
        "\n",
        "def travel_idea(interest, budget):\n",
        "  system_template = \"You are a travel guide. Your job is to give people travel ideas based on their interest and budget. Give a daily plan for a two week travel.\"\n",
        "  human_template = \"I'm interested {interest} and my budget is around {budget}\"\n",
        "\n",
        "  system_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "  human_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "  chat_prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
        "\n",
        "  chat = ChatOpenAI()\n",
        "\n",
        "  result = chat(chat_prompt.format_messages(interest=interest, budget=budget))\n",
        "  return result.content\n",
        "\n",
        "print(travel_idea(\"fishing\", \"$10,000\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abUjUGRFOTW1",
        "outputId": "6a4e08f2-2784-4206-fc87-d1ee10b22333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you're interested in fishing and have a budget of $10,000, here's a two-week travel plan that will provide you with some amazing fishing experiences:\n",
            "\n",
            "Week 1:\n",
            "\n",
            "Day 1-3: Alaska, USA\n",
            "Fly into Anchorage and head to one of the renowned fishing lodges in Alaska, such as the Kenai River or Bristol Bay. Spend three days exploring the pristine waters, fishing for salmon, trout, and halibut.\n",
            "\n",
            "Day 4-6: Cairns, Australia\n",
            "Fly to Cairns, known as the gateway to the Great Barrier Reef. Charter a fishing boat and spend three days fishing in the world's largest coral reef system. Expect to catch a variety of fish, including marlin, tuna, and barracuda.\n",
            "\n",
            "Day 7: Travel Day\n",
            "Take a day to travel from Cairns to your next destination. You can either fly directly or make a short stopover in a city of your choice.\n",
            "\n",
            "Week 2:\n",
            "\n",
            "Day 8-10: Tromso, Norway\n",
            "Fly to Tromso, located in the Arctic Circle. Experience fishing in the stunning fjords and Arctic waters. This region is known for its abundance of cod, halibut, and salmon. Consider booking a fishing tour that includes the opportunity to see the Northern Lights.\n",
            "\n",
            "Day 11-12: Punta Gorda, Belize\n",
            "Fly to Belize and head to Punta Gorda, a fishing hotspot known for its diverse marine life. Spend two days fishing in the crystal-clear waters, targeting species like bonefish, tarpon, and permit. You can also explore the nearby Belize Barrier Reef if you have time.\n",
            "\n",
            "Day 13-14: San Diego, USA\n",
            "Fly to San Diego and spend the last two days of your trip fishing in California. Charter a boat and try your luck at catching yellowtail, tuna, and other species found in the Pacific Ocean. Enjoy the vibrant coastal city and its delicious seafood.\n",
            "\n",
            "Remember, the above plan includes travel and fishing expenses. Prices may vary, so it's advisable to research and book in advance to get the best deals. Enjoy your fishing adventure!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fewshot PromptTemplates\n",
        "\n",
        "* Sometimes it's easier to give the LLM a few examples of input/output pairs before sending your main request.\n",
        "* This allows the LLM to \"learn\" the pattern you are looking for and may lead to better results.\n",
        "* It should be noted that there is currently no consensus on best practices, but Langchain recommends building a history of Human and AI messages inputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "06jbFDwHQo7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate\n",
        "\n",
        "# AI Bot to explain legal documents in simple terms\n",
        "\n",
        "system_prompt = SystemMessagePromptTemplate.from_template(\n",
        "    \"You are a helpful legal assistant that translates complex legal terms into plain and understandable terms.\"\n",
        ")\n",
        "\n",
        "legal_text = \"The provisions herein shall be severable, and if any provision or protion thereof is deemed invalid, illegal, or unenforceable by a court of competent jurisdiction, the remaining provisions or portions thereof shall remain in full force and effect to the maximum extent permitted by law.\"\n",
        "example_input_1 = HumanMessagePromptTemplate.from_template(legal_text)\n",
        "\n",
        "plain_text = \"Thhe rules in this agreement can be separated.\"\n",
        "example_output_1 = AIMessagePromptTemplate.from_template(plain_text)\n",
        "\n",
        "human_prompt = HumanMessagePromptTemplate.from_template(\"{legal_text}\")\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    system_prompt,\n",
        "    example_input_1,\n",
        "    example_output_1,\n",
        "    human_prompt\n",
        "])\n",
        "\n",
        "legal_text = \"Many jurisdictions retain the possibility of creating a life estate, although this is uncommon. In the United States, life estates are most commonly used either to grant someone use of the property for the remainder of that person's life in a will, or by a grantor to reserve the right to continue using the property for the remainder of the grantor's life after it is sold. The right to ownership of the property after the death of the life estate owner is called the remainder estate. In England and Wales fee simple is the only freehold estate that remains; a life estate can only be created in equity and is not a right in property.\"\n",
        "\n",
        "chat = ChatOpenAI()\n",
        "result = chat(chat_prompt.format_messages(legal_text=legal_text))\n",
        "print(result.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG_nwU_uU-pd",
        "outputId": "653a6e88-b004-4263-94e2-81a04699cd11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In some places, it is still possible to create a life estate, although this is not very common. In the United States, life estates are often used in two ways: either to allow someone to use a property for the rest of their life through a will, or for a property owner to keep using the property after selling it until they pass away. The ownership of the property after the life estate owner dies is called the remainder estate. In England and Wales, fee simple is the only type of ownership that exists, and a life estate can only be created through equity and is not a full right to the property.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing Outputs\n",
        "\n",
        "* Often when connecting LLM output you need it in a particular format, for example, you want a python datetime object, or a JSON object.\n",
        "* Langchain comes with Parse utilities allowing you to easily convert outputs into precise data types or even your own custom class intances with Pydantic.\n",
        "\n",
        "Parsers consist of two key elements:\n",
        "* `format_instructions`: An extra string that langchain adds to the end of a prompt to assist with formatting\n",
        "* `parse()` method: a method for using `eval()` internally to parse the string reply to the exact Python object you need.\n",
        "\n",
        "### Parser Example\n",
        "\n",
        "Suppose you need a datetime repose from LLM:\n",
        "* Two main issues:\n",
        "  * LLM always replies back with a string e.g. `2020-01-01`\n",
        "  * Could be formatted in many ways: `Jan 1st, 2020`\n",
        "* Parsers use `format_instructions` to take care of the first issue and `eval()` to take care of the second issue.\n",
        "* For this we can use `DatetimeOutputParser`\n",
        "  * Replies are actual datetime objects after using `parse()`\n",
        "* You can also use `AutoFix` with `OutputFixParser` to re-attemp the correct parsed output with another LLM."
      ],
      "metadata": {
        "id": "wHwTnm6iTKK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "print(\"Format Instructions: \", output_parser.get_format_instructions())\n",
        "\n",
        "human_prompt = HumanMessagePromptTemplate.from_template(\"{request}\\n\\n{format_instructions}\")\n",
        "chat_prompt = ChatPromptTemplate(\n",
        "    messages=[human_prompt],\n",
        "    input_variables=[\"request\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        "    # output_parser=output_parser # LLMChain will not automatically call this parser. Should use one of the *_and_parse methods instead of run\n",
        ")\n",
        "\n",
        "request=\"give me 5 characteristics of dogs\"\n",
        "\n",
        "print(\"Fomatted Prompt: \", chat_prompt.format(request=request))\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "chain = LLMChain(llm=llm, prompt=chat_prompt, output_parser=output_parser)\n",
        "\n",
        "result = chain.run(request=request)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "EqXzxgGkVPXE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dcec702-c0fe-4dff-c53e-4f788ecabd08"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Format Instructions:  Your response should be a list of comma separated values, eg: `foo, bar, baz`\n",
            "Fomatted Prompt:  Human: give me 5 characteristics of dogs\n",
            "\n",
            "Your response should be a list of comma separated values, eg: `foo, bar, baz`\n",
            "['Loyal', 'playful', 'protective', 'trainable', 'social']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've seen the basics on how to use parsers, but what happens when that still isn't enough to format your outputs?\n",
        "\n",
        "There are two ways to solve this\n",
        "* System Prompt: Have a strong system prompt to combine with your format instructions.\n",
        "* `OutputFixingParser`: Using a chain, re-send your original reply to an LLM to try to fix it."
      ],
      "metadata": {
        "id": "TU1ozfevPHM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import DatetimeOutputParser, OutputFixingParser\n",
        "\n",
        "output_parser = DatetimeOutputParser()\n",
        "\n",
        "system_prompt = SystemMessagePromptTemplate.from_template(\"You always reply to questions only in datetime patterns.\")\n",
        "human_prompt = HumanMessagePromptTemplate.from_template(\"{request}\\n{format_instructions}\")\n",
        "\n",
        "chat_prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        system_prompt,\n",
        "        human_prompt\n",
        "    ],\n",
        "    input_variables=[\"request\"],\n",
        "    partial_variables={\n",
        "        \"format_instructions\": output_parser.get_format_instructions()\n",
        "    }\n",
        ")\n",
        "\n",
        "model_request = chat_prompt.format_messages(request=\"What date was the 13th Amendment ratified in the US\")\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "result = llm(model_request)\n",
        "print(\"Result Content: \", result.content)\n",
        "\n",
        "output_parser.parse(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKt1VUw2Q06R",
        "outputId": "f95278e1-09e9-4ba0-8df9-87f2a33f9a41"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result Content:  1865-12-06T00:00:00.000000Z\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.datetime(1865, 12, 6, 0, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PydanticOutputParser\n",
        "\n",
        "Using the Pydantic library for type validation, you can use Langchain's **PydanticOutputParser** to directly attempt to convert LLM replies to your own custom python objects (as long as you build them with Pydantic).\n"
      ],
      "metadata": {
        "id": "cJuY9voYbDyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Scientist(BaseModel):\n",
        "  name: str = Field(description=\"Name of a scientist\")\n",
        "  discoveries: list = Field(description=\"Python list of discoveries\")\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Scientist)\n",
        "\n",
        "print(\"Format Instructions: \", parser.get_format_instructions())\n",
        "\n",
        "human_prompt = HumanMessagePromptTemplate.from_template(\"{request}\\n{format_instructions}\")\n",
        "chat_prompt = ChatPromptTemplate(\n",
        "    messages=[human_prompt],\n",
        "    input_variables=[\"request\"],\n",
        "    partial_variables={\n",
        "        \"format_instructions\": parser.get_format_instructions()\n",
        "    }\n",
        ")\n",
        "\n",
        "request = chat_prompt.format_messages(request=\"Tell me about a famous scientist\")\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "result = llm(request)\n",
        "parser.parse(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjR6XlwBbbV6",
        "outputId": "c9fbcbb1-1f68-45a7-9035-e671894ced25"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Format Instructions:  The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Name of a scientist\", \"type\": \"string\"}, \"discoveries\": {\"title\": \"Discoveries\", \"description\": \"Python list of discoveries\", \"type\": \"array\", \"items\": {}}}, \"required\": [\"name\", \"discoveries\"]}\n",
            "```\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Scientist(name='Albert Einstein', discoveries=['Theory of Relativity', 'Photoelectric Effect'])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Serialization\n",
        "\n",
        "* You may find yourself wanting to save, share, or load prompt objects.\n",
        "* Langchain allows you to easily save Prompt templates as JSON files to read or share.\n",
        "* Let's explore this further with some examples\n",
        "\n"
      ],
      "metadata": {
        "id": "Z_7np_W2eZJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.prompts import load_prompt\n",
        "\n",
        "template = \"Tell me a fact about {planet}\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "print(prompt)\n",
        "\n",
        "prompt.save(\"my_prompt.json\")\n",
        "\n",
        "loaded_prompt = load_prompt(\"my_prompt.json\")\n",
        "print(loaded_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWpN3miyemql",
        "outputId": "bd3a0c0d-aff8-4557-d1cb-e38a9fcd2afd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['planet'] output_parser=None partial_variables={} template='Tell me a fact about {planet}' template_format='f-string' validate_template=True\n",
            "input_variables=['planet'] output_parser=None partial_variables={} template='Tell me a fact about {planet}' template_format='f-string' validate_template=True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inputs and Outputs Excercise\n",
        "\n",
        "The purpose of this exercise is to test your understanding of building out Model IO systems. You will hopefully notice the need to chain responses together, which we will conver later on!\n",
        "\n",
        "Our main goal is to use Langchain and Python to create a very simple class with a few methods for:\n",
        "* Writing a historical question that has a date as the correct answer\n",
        "* Getting the correct answer from LLM\n",
        "* Getting a Human user's best guess at correct answer\n",
        "* Checking/reporting the difference between the correct answer and the user answer"
      ],
      "metadata": {
        "id": "u5W_7tE7fItq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "from datetime import datetime\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.output_parsers import DatetimeOutputParser\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "class HistoryQuiz:\n",
        "  def create_history_question(self, topic):\n",
        "    '''\n",
        "    This method should output a historical question about the topic that has a date as the\n",
        "    For example:\n",
        "\n",
        "      \"On what date did World War 2 end?\"\n",
        "\n",
        "    '''\n",
        "\n",
        "    system_template = \"You write single quiz question about {topic}. You only return the quiz question.\"\n",
        "    system_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "\n",
        "    human_template = \"{question_request}\"\n",
        "    human_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "    chat_prompt = ChatPromptTemplate.from_messages([\n",
        "        system_prompt,\n",
        "        human_prompt\n",
        "    ])\n",
        "\n",
        "    q = \"Give me a quiz question where the correct answer is a specific date.\"\n",
        "    request = chat_prompt.format_messages(topic=topic, question_request=q)\n",
        "\n",
        "    chat = ChatOpenAI()\n",
        "    result = chat(request)\n",
        "\n",
        "    return result.content\n",
        "\n",
        "  def get_ai_answer(self, question):\n",
        "    '''\n",
        "    This method should get the answer to the historical question from the method above.\n",
        "    Note: This answer must be in datetime format! Use DatetimeOutputParser to confirm!\n",
        "\n",
        "    September 2, 1945 --> datetime.datetime(1945, 9, 2, 0, 0)\n",
        "    '''\n",
        "\n",
        "    output_parser = DatetimeOutputParser()\n",
        "\n",
        "    system_template = \"You answer quiz questions with just a date.\"\n",
        "    system_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "\n",
        "    human_template = \"\"\"Answer the user's question:\n",
        "\n",
        "    {question}\n",
        "\n",
        "    {format_instructions}\"\"\"\n",
        "    human_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "    chat_prompt = ChatPromptTemplate(\n",
        "        messages=[system_prompt, human_prompt],\n",
        "        input_variables=[\"question\"],\n",
        "        partial_variables={\n",
        "            \"format_instructions\": output_parser.get_format_instructions()\n",
        "        }\n",
        "    )\n",
        "\n",
        "    request = chat_prompt.format_messages(question=question)\n",
        "\n",
        "    chat = ChatOpenAI()\n",
        "    result = chat(request)\n",
        "\n",
        "    datetime = output_parser.parse(result.content)\n",
        "\n",
        "    return datetime\n",
        "\n",
        "  def get_user_answer(self, question):\n",
        "    '''\n",
        "    This method should grab a user answer and convert it to datetime. It should collect a\n",
        "    You can just use input() for this.\n",
        "    '''\n",
        "\n",
        "    print(question)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    year = int(input(\"Enter the year: \"))\n",
        "    month = int(input(\"Enter the month (1-12): \"))\n",
        "    day = int(input(\"Enter the day (1-31): \"))\n",
        "\n",
        "    user_datetime = datetime(year, month, day)\n",
        "\n",
        "    return user_datetime\n",
        "\n",
        "  def check_user_answer(self, user_answer, ai_answer):\n",
        "    '''\n",
        "    Should check the user answer against the AI answer and return the difference between\n",
        "    '''\n",
        "\n",
        "    difference = user_answer - ai_answer\n",
        "    formatted_difference = str(difference)\n",
        "    print(\"The difference between the answer an you guess: \", formatted_difference)\n",
        "\n",
        "quiz_bot = HistoryQuiz()\n",
        "\n",
        "question = quiz_bot.create_history_question(topic='World War 2')\n",
        "print(\"Question: \", question)\n",
        "\n",
        "ai_answer = quiz_bot.get_ai_answer(question)\n",
        "print(ai_answer)\n",
        "\n",
        "user_answer = quiz_bot.get_user_answer(question)\n",
        "print(user_answer)\n",
        "\n",
        "quiz_bot.check_user_answer(user_answer, ai_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6he4PQgjHhI",
        "outputId": "eafac337-36b0-4d0e-9451-1d993a56a7eb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  On what date did the attack on Pearl Harbor occur, drawing the United States into World War II?\n",
            "1941-12-07 08:00:00\n",
            "On what date did the attack on Pearl Harbor occur, drawing the United States into World War II?\n",
            "\n",
            "\n",
            "Enter the year: 1942\n",
            "Enter the month (1-12): 1\n",
            "Enter the day (1-31): 7\n",
            "1942-01-07 00:00:00\n",
            "The difference between the answer an you guess:  30 days, 16:00:00\n"
          ]
        }
      ]
    }
  ]
}