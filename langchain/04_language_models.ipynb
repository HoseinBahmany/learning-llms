{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjFnmee8BeX1rAp+RQIEHC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoseinBahmany/learning-llms/blob/main/langchain/04_language_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZuE-sKSd9uW",
        "outputId": "3d36bef4-6e84-4f55-9897-35c6fba82c23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.238)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.18)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.5.13)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.12)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.11)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.5.3)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.1)\n",
            "Requirement already satisfied: fastapi<0.100.0,>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.99.1)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.23.1)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.7.1)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.2.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.15.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.13.3)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.3.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi<0.100.0,>=0.95.2->chromadb) (0.27.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.11.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.4)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (3.7.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain openai chromadb tiktoken numpy faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"1516792b8aa8d598271fd69823f3590da610d429c776fff1deca86f4415bc818\""
      ],
      "metadata": {
        "id": "NBUTzgUQeFsZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain provides interfaces and integrations for two types of models:\n",
        "\n",
        "* **LLMs**: Models that take a text string as input and return a text string\n",
        "* **Chat models**: Models that are backed by a language model but take a list of Chat Messages as input and return a Chat Message"
      ],
      "metadata": {
        "id": "YOztaMfVe1Rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLMs vs Chat Models\n",
        "\n",
        "LLMs and Chat Models are subtly but importantly different. LLMs in LangChain refer to pure text completion models. The APIs they wrap take a string prompt as input and output a string completion. OpenAI's GPT-3 is implemented as an LLM. Chat models are often backed by LLMs but tuned specifically for having conversations. And, crucially, their provider APIs expose a different interface than pure text completion models. Instead of a single string, they take a list of chat messages as input. Usually these messages are labeled with the speaker (usually one of \"System\", \"AI\", and \"Human\"). And they return a (\"AI\") chat message as output. GPT-4 and Anthropic's Claude are both implemented as Chat Models.\n",
        "\n",
        "To make it possible to swap LLMs and Chat Models, both implement the Base Language Model interface. This exposes common methods \"predict\", which takes a string and returns a string, and \"predict messages\", which takes messages and returns a message. If you are using a specific model it's recommended you use the methods specific to that model class (i.e., \"predict\" for LLMs and \"predict messages\" for Chat Models), but if you're creating an application that should work with different types of models the shared interface can be helpful."
      ],
      "metadata": {
        "id": "adlj7Nyce8Q8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom LLM\n",
        "\n",
        "Let's see how to create a custom LLM wrapper, in case you want to use your own LLM or a different wrapper than one that is supported in LangChain.\n",
        "\n",
        "There is only one required thing that a custom LLM needs to implement:\n",
        "\n",
        "1. A `_call` method that takes in a string, some optional stop words, and returns a string\n",
        "\n",
        "There is a second optional thing it can implement:\n",
        "\n",
        "2. An `_identifying_params` property that is used to help with printing of this class. Should return a dictionary.\n",
        "\n",
        "Let's implement a very simple custom LLM that just returns the first N characters of the input."
      ],
      "metadata": {
        "id": "Vkm0-SQGfeEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, List, Mapping, Optional\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain.llms.base import LLM\n",
        "\n",
        "class CustomLLM(LLM):\n",
        "  n: int\n",
        "\n",
        "  @property\n",
        "  def _llm_type(self) -> str:\n",
        "    return \"Custom\"\n",
        "\n",
        "  def _call(\n",
        "      self,\n",
        "      prompt: str,\n",
        "      stop: Optional[List[str]] = None,\n",
        "      run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "  ) -> str:\n",
        "    if stop is not None:\n",
        "      raise ValueError(\"stop kwargs are not permitted\")\n",
        "    return prompt[:self.n]\n",
        "\n",
        "  @property\n",
        "  def _identifying_params(self) -> Mapping[str, Any]:\n",
        "    \"\"\"Get the identifying parameters.\"\"\"\n",
        "    return {\"n\": self.n}\n",
        "\n",
        "llm = CustomLLM(n=10)\n",
        "\n",
        "print(llm(\"This is a foobar string\"))\n",
        "\n",
        "print(llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llXL-teLfyFQ",
        "outputId": "b4d3dca7-54ca-4701-bd4a-c1602654e522"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a \n",
            "\u001b[1mCustomLLM\u001b[0m\n",
            "Params: {'n': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Caching\n",
        "\n",
        "LangChain provides an optional caching layer for LLMs. This is useful for two reasons:\n",
        "\n",
        "It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times. It can speed up your application by reducing the number of API calls you make to the LLM provider."
      ],
      "metadata": {
        "id": "0FI7BYJxhBrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"text-davinci-002\", best_of=2)"
      ],
      "metadata": {
        "id": "aPTeZV2RhK-W"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In Memory Cache"
      ],
      "metadata": {
        "id": "_Pnw3U_ehacY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.cache import InMemoryCache\n",
        "\n",
        "langchain.chache = InMemoryCache()\n",
        "\n",
        "print(llm.predict(\"Tell me a joke\"))\n",
        "\n",
        "# The second time it is, so it goes faster\n",
        "print(llm.predict(\"Tell me a joke\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zhg2nG2uhcNW",
        "outputId": "2308e5cf-567a-4fb1-91d7-5d57dc55d0a1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Why did the chicken cross the road?\n",
            "\n",
            "To get to the other side.\n",
            "\n",
            "\n",
            "Why did the chicken cross the road?\n",
            "\n",
            "To get to the other side!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SQLite Cache"
      ],
      "metadata": {
        "id": "Xp6_iNJIiH1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm .langchain.db\n",
        "\n",
        "from langchain.cache import SQLiteCache\n",
        "langchain.llm_cache = SQLiteCache(database_path=\".langchain.db\")\n",
        "\n",
        "print(llm.predict(\"Tell me a joke\"))\n",
        "\n",
        "# The second time it is, so it goes faster\n",
        "print(llm.predict(\"Tell me a joke\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LNIdBf1AiLAH",
        "outputId": "fb1b6ddb-4ae5-4fc6-f2a5-4b2a2816f100"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nA man walks into a bar and asks for a beer. The bartender says \"You\\'re out of luck. We\\'ve been closed for fifteen minutes.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming Response\n",
        "\n",
        "Some LLMs provide a streaming response. This means that instead of waiting for the entire response to be returned, you can start processing it as soon as it's available. This is useful if you want to display the response to the user as it's being generated, or if you want to process the response as it's being generated.\n",
        "\n",
        "Currently, we support streaming for the `OpenAI`, `ChatOpenAI`, and `ChatAnthropic` implementations. To utilize streaming, use a `CallbackHandler` that implements `on_llm_new_token`. In this example, we are using `StreamingStdOutCallbackHandler`."
      ],
      "metadata": {
        "id": "dvZe_WNQisy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "llm = OpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)\n",
        "resp = llm(\"Write me a song about sparking waters.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RvguJmzi314",
        "outputId": "288145b7-7646-431a-b6e7-9ccc41699f81"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Verse 1\n",
            "\n",
            "I'm standing here, by the sparkling waters\n",
            "So clear and blue, like a diamond in the sky\n",
            "The sun is shining, the birds are singing\n",
            "A perfect day, to just sit and watch the tide\n",
            "\n",
            "Chorus\n",
            "\n",
            "Sparkling waters, so beautiful and pure\n",
            "A sight to behold, a sight to endure\n",
            "The waves are crashing, the wind is blowing\n",
            "A peaceful place, to just sit and watch the shore\n",
            "\n",
            "Verse 2\n",
            "\n",
            "The sand is warm, the air is sweet\n",
            "The smell of salt, it's so hard to beat\n",
            "The waves are rolling, the seagulls calling\n",
            "A perfect day, to just sit and watch the sea\n",
            "\n",
            "Chorus\n",
            "\n",
            "Sparkling waters, so beautiful and pure\n",
            "A sight to behold, a sight to endure\n",
            "The waves are crashing, the wind is blowing\n",
            "A peaceful place, to just sit and watch the shore\n",
            "\n",
            "Bridge\n",
            "\n",
            "The sun is setting, the day is done\n",
            "The stars are twinkling, the night has come\n",
            "The moon is shining, the sky is bright\n",
            "A perfect night, to just sit and watch the light\n",
            "\n",
            "Chorus\n",
            "\n",
            "Sparkling waters, so"
          ]
        }
      ]
    }
  ]
}