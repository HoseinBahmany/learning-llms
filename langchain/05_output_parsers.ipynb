{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBjeCbMV3Vec1sDkV6vPSz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoseinBahmany/learning-llms/blob/main/langchain/05_output_parsers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9sjh09zj4WN"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai chromadb tiktoken numpy faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"1516792b8aa8d598271fd69823f3590da610d429c776fff1deca86f4415bc818\""
      ],
      "metadata": {
        "id": "gM6hOjCRJxNl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.\n",
        "\n",
        "Output parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\n",
        "\n",
        "* \"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
        "* \"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n",
        "\n",
        "And then one optional one:\n",
        "\n",
        "* \"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so."
      ],
      "metadata": {
        "id": "vYfC27ojJyCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pydantic (JSON) parser\n",
        "\n",
        "This output parser allows users to specify an arbitrary JSON schema and query LLMs for JSON outputs that conform to that schema.\n",
        "\n",
        "Keep in mind that large language models are leaky abstractions! You'll have to use an LLM with sufficient capacity to generate well-formed JSON. In the OpenAI family, DaVinci can do reliably but Curie's ability already drops off dramatically.\n",
        "\n",
        "Use Pydantic to declare your data model. Pydantic's BaseModel like a Python dataclass, but with actual type checking + coercion."
      ],
      "metadata": {
        "id": "3gkl0fPlJ7ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from typing import List\n",
        "\n",
        "model_name = \"text-davinci-003\"\n",
        "temperature = 0.0\n",
        "model = OpenAI(model=model_name, temperature=temperature)\n",
        "\n",
        "# Define the desired data structure\n",
        "class Joke(BaseModel):\n",
        "  setup: str = Field(description=\"question to setup a joke\")\n",
        "  punchline: str = Field(description=\"answer to resolve the joke\")\n",
        "\n",
        "  @validator('setup')\n",
        "  def question_ends_with_question_mark(cls, field):\n",
        "    if field[-1] != '?':\n",
        "      raise ValueError(\"Badly formatted question!\")\n",
        "    return field\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Joke)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "joke_query = \"Tell me a joke\"\n",
        "print(prompt.format(query=joke_query))\n",
        "\n",
        "output = model(prompt.format(query=joke_query))\n",
        "\n",
        "parsed_obj = parser.parse(output)\n",
        "\n",
        "print(\"parsed object: \", parsed_obj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSNP-L2CJ_Wu",
        "outputId": "d8bae6f1-c2c4-41b4-c916-a97e3e079a0d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the user query.\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to setup a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
            "```\n",
            "Tell me a joke\n",
            "\n",
            "parsed object:  setup='Why did the chicken cross the road?' punchline='To get to the other side!'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List Parser\n",
        "\n",
        "This output parser can be used when you want to return a list of comma-separated items."
      ],
      "metadata": {
        "id": "b6xBM1sgNF1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Live five {subject}.\\n{format_instructions}\",\n",
        "    input_variables=[\"subject\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "input = prompt.format(subject=\"ice cream flavors\")\n",
        "output = llm(input)\n",
        "parsed_output = output_parser.parse(output)\n",
        "\n",
        "print(\"input: \", input)\n",
        "print(\"output: \", output)\n",
        "print(\"parsed_output: \", parsed_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lboRtQHDNMZP",
        "outputId": "79860733-efed-4520-b6f4-bad625193746"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:  Live five ice cream flavors.\n",
            "Your response should be a list of comma separated values, eg: `foo, bar, baz`\n",
            "output:  \n",
            "\n",
            "Vanilla, Chocolate, Strawberry, Mint Chocolate Chip, Cookies and Cream\n",
            "parsed_output:  ['Vanilla', 'Chocolate', 'Strawberry', 'Mint Chocolate Chip', 'Cookies and Cream']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DateTime Parser\n",
        "\n",
        "This OutputParser shows out to parse LLM output into datetime format."
      ],
      "metadata": {
        "id": "J3Jn9-maOnHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.output_parsers import DatetimeOutputParser\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "output_parser = DatetimeOutputParser()\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"Answer the user question:\\n\\n{question}\\n\\n{format_instructions}\",\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "chain = LLMChain(prompt=prompt, llm=OpenAI(temperature=0), output_parser=output_parser)\n",
        "\n",
        "output = chain.run(\"Around when was Bitcoin founded?\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkRO2dZkOlUa",
        "outputId": "853f8316-a8e3-4884-ba1f-a96643aa8c6b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2008-01-03 18:15:05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enum Parser"
      ],
      "metadata": {
        "id": "eXCxOrxePiuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.output_parsers import EnumOutputParser\n",
        "from langchain.llms import OpenAI\n",
        "from enum import Enum\n",
        "\n",
        "class Colors(Enum):\n",
        "  POSITIVE = \"Positive\"\n",
        "  NEGATIVE = \"Negative\"\n",
        "\n",
        "output_parser = EnumOutputParser(enum=Colors)\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"Determine the sentiment of the user's sentence.\\n{format_instructions}\\nsentence: {sentence}\",\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "input = prompt.format(sentence=\"Today I had a very rough day. My boss just kept berating me and I was so stressed out!\")\n",
        "output = llm(input)\n",
        "parsed_output = output_parser.parse(output)\n",
        "\n",
        "print(\"input: \", input)\n",
        "print(\"output: \", output)\n",
        "print(\"parsed_output: \", parsed_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oVDOfnKPmm3",
        "outputId": "b5ff8337-e434-47c4-ca4d-a9ca602333e3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:  Determine the sentiment of the user's sentence.\n",
            "Select one of the following options: Positive, Negative\n",
            "sentence: Today I had a very rough day. My boss just kept berating me and I was so stressed out!\n",
            "output:  \n",
            "Negative\n",
            "parsed_output:  Colors.NEGATIVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auto-fixing Parser\n",
        "\n",
        "This output parser wraps another output parser, and in the event that the first one fails it calls out to another LLM to fix any errors.\n",
        "\n",
        "But we can do other things besides throw errors. Specifically, we can pass the misformatted output, along with the formatted instructions, to the model and ask it to fix it."
      ],
      "metadata": {
        "id": "kD7C2lduRk5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.output_parsers import PydanticOutputParser, OutputFixingParser\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from typing import List\n",
        "\n",
        "class Actor(BaseModel):\n",
        "  name: str = Field(description=\"name of an actor\")\n",
        "  film_names: List[str] = Field(description=\"list of names of films they starred in\")\n",
        "\n",
        "pydantic_parser = PydanticOutputParser(pydantic_object=Actor)\n",
        "\n",
        "output_parser = OutputFixingParser.from_llm(parser=pydantic_parser, llm=ChatOpenAI(temperature=0))\n",
        "\n",
        "misformatted = \"{'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}\"\n",
        "\n",
        "print(output_parser.parse(misformatted))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "234MMzG0RpOX",
        "outputId": "82783536-8c56-44c4-9b0f-6a4cd94c643a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='Tom Hanks' film_names=['Forrest Gump']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retry parser\n",
        "\n",
        "While in some cases it is possible to fix any parsing mistakes by only looking at the output, in other cases it can't. An example of this is when the output is not just in the incorrect format, but is partially complete. Consider the below example.\n",
        "\n"
      ],
      "metadata": {
        "id": "t3Ss-npkUatO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.output_parsers import PydanticOutputParser, RetryWithErrorOutputParser\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from typing import List\n",
        "\n",
        "class Action(BaseModel):\n",
        "  action: str = Field(description=\"action to take\")\n",
        "  action_input: str = Field(description=\"input to the action\")\n",
        "\n",
        "pydantic_parser = PydanticOutputParser(pydantic_object=Action)\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"Based on the user question, provide an Action and Action Input for what step should be taken.\\n{format_instructions}\\nQuestion: {query}\\nResponse: \",\n",
        "    partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "prompt_value = prompt.format_prompt(query=\"Who is Leo Di Caprio's GF?\")\n",
        "\n",
        "# suppose the LLM has returned a bad and partial response like this\n",
        "bad_response = '{\"action\": \"search\"}'\n",
        "\n",
        "# This would fail because the required field \"action_input\" is missing\n",
        "# pydantic_parser.parse(bad_response)\n",
        "\n",
        "retry_parser = RetryWithErrorOutputParser.from_llm(\n",
        "    llm=OpenAI(temperature=0),\n",
        "    parser=pydantic_parser\n",
        ")\n",
        "\n",
        "retry_parser.parse_with_prompt(bad_response, prompt_value)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDHt_QwHUcSM",
        "outputId": "55ba7812-d4c5-420a-9db3-a02902d36dff"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Action(action='search', action_input=\"Leo Di Caprio's GF\")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}