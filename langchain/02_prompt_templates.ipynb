{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMaryd9gML1lgVAtY9IQ6c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoseinBahmany/learning-llms/blob/main/langchain/02_prompt_templates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai chromadb tiktoken"
      ],
      "metadata": {
        "id": "tSKo3N8dcJCO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f898344f-7ae3-47e8-b5e1-79011f5d65ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.238-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb\n",
            "  Downloading chromadb-0.4.2-py3-none-any.whl (399 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.3/399.3 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.18)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.12-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.11 (from langchain)\n",
            "  Downloading langsmith-0.0.11-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.11)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.5.3)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chroma-hnswlib==0.7.1 (from chromadb)\n",
            "  Downloading chroma-hnswlib-0.7.1.tar.gz (30 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastapi<0.100.0,>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.23.1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.1-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.7.1)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.2 (from chromadb)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.3.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi<0.100.0,>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.11.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.4)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.1.2)\n",
            "Building wheels for collected packages: chroma-hnswlib, pypika\n",
            "  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.1-cp310-cp310-linux_x86_64.whl size=2273171 sha256=e1c4735e93f8559caae7cd6d0f54870ad0ee7bf8df6a57eed440d619ae7fb8de\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/f2/d2/3f32228e9f4713a9f32a468de8bbc3c642f7805ebef888418b\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=f343a81513f13292c8e3eadb9b239b321f56017abd046c1389836d82841f4594\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built chroma-hnswlib pypika\n",
            "Installing collected packages: tokenizers, pypika, monotonic, websockets, uvloop, requests, python-dotenv, pulsar-client, overrides, mypy-extensions, marshmallow, humanfriendly, httptools, h11, chroma-hnswlib, backoff, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, posthog, openapi-schema-pydantic, langsmith, coloredlogs, openai, onnxruntime, fastapi, dataclasses-json, langchain, chromadb\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 chroma-hnswlib-0.7.1 chromadb-0.4.2 coloredlogs-15.0.1 dataclasses-json-0.5.12 fastapi-0.99.1 h11-0.14.0 httptools-0.6.0 humanfriendly-10.0 langchain-0.0.238 langsmith-0.0.11 marshmallow-3.19.0 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.15.1 openai-0.27.8 openapi-schema-pydantic-1.2.4 overrides-7.3.1 posthog-3.0.1 pulsar-client-3.2.0 pypika-0.48.9 python-dotenv-1.0.0 requests-2.31.0 starlette-0.27.0 tiktoken-0.4.0 tokenizers-0.13.3 typing-inspect-0.9.0 uvicorn-0.23.1 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"1516792b8aa8d598271fd69823f3590da610d429c776fff1deca86f4415bc818\""
      ],
      "metadata": {
        "id": "i57S4VTzcmTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt templates\n",
        "\n",
        "Language models take text as input - that text is commonly referred to as a prompt. Typically this is not simply a hardcoded string but rather a combination of a template, some examples, and user input. LangChain provides several classes and functions to make constructing and working with prompts easy."
      ],
      "metadata": {
        "id": "BI4f1eMMbxjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is a Prompt Template?\n",
        "\n",
        "A prompt template refers to a reproducible way to generate a prompt. It contains a text string (\"the template\"), that can take in a set of parameters from the end user and generates a prompt.\n",
        "\n",
        "A prompt template can contain:\n",
        "\n",
        "instructions to the language model,\n",
        "a set of few shot examples to help the language model generate a better response,\n",
        "a question to the language model.\n",
        "Here's the simplest example:"
      ],
      "metadata": {
        "id": "5tQZ95-nb3mp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CADL6_DVbq8g",
        "outputId": "699932e4-d6da-4194-ca2e-64afa1ae06c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/\\nYou are a naming consultant for new companies.\\nWhat is a good name for a company that makes Colorful Socks?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template =\"\"\"/\n",
        "You are a naming consultant for new companies.\n",
        "What is a good name for a company that makes {product}?\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "prompt.format(product=\"Colorful Socks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Prompt Template\n",
        "\n",
        "You can create simple hardcoded prompts using the PromptTemplate class. Prompt templates can take any number of input variables, and can be formatted to generate a prompt."
      ],
      "metadata": {
        "id": "tI3WGtYOdNDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "no_input_prompt = PromptTemplate(input_variables=[], template=\"Tell me a joke.\")\n",
        "print(no_input_prompt.format())\n",
        "\n",
        "one_input_prompt = PromptTemplate(input_variables=[\"adjective\"], template=\"Tell me a {adjective} joke.\")\n",
        "print(one_input_prompt.format(adjective=\"funny\"))\n",
        "\n",
        "multi_input_prompt = PromptTemplate(\n",
        "    input_variables=[\"adjective\", \"content\"],\n",
        "    template=\"Tell a {adjective} joke about {content}.\"\n",
        ")\n",
        "print(multi_input_prompt.format(adjective=\"funny\", content=\"chickens\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AADpZgJdaX9",
        "outputId": "f0f4898e-1a7e-4513-9fd1-0f8c485637b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell me a joke.\n",
            "Tell me a funny joke.\n",
            "Tell a funny joke about chickens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Prompt Template\n",
        "\n",
        "Chat Models take a list of chat messages as input - this list commonly referred to as a prompt. These chat messages differ from raw string (which you would pass into a LLM model) in that every message is associated with a role.\n",
        "\n",
        "For example, in OpenAI Chat Completion API, a chat message can be associated with the AI, human or system role. The model is supposed to follow instruction from system chat message more closely.\n",
        "\n",
        "LangChain provides several prompt templates to make constructing and working with prompts easily. You are encouraged to use these chat related prompt templates instead of `PromptTemplate` when querying chat models to fully exploit the potential of underlying chat model."
      ],
      "metadata": {
        "id": "2JR4GQ-5eZMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(\n",
        "    \"You are a helpful assistant that translates {input_language} to {output_language}\"\n",
        ")\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(\"{text}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "iXIcWWHie7xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After that, you can build a `ChatPromptTemplate` from one or more `MessagePromptTemplates`. You can use `ChatPromptTemplate`'s `format_prompt` -- this returns a `PromptValue`, which you can convert to a `string` or `Message` object, depending on whether you want to use the formatted value as input to an llm or chat model."
      ],
      "metadata": {
        "id": "wjEiC_xJfSza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "# get a chat completion from the formatted messages\n",
        "chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQHzujfjfglU",
        "outputId": "228958a7-3e79-40d1-87d8-c109d43d1d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are a helpful assistant that translates English to French', additional_kwargs={}),\n",
              " HumanMessage(content='I love programming.', additional_kwargs={}, example=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Prompt Templates\n",
        "\n",
        "## Why are custom prompt templates needed?\n",
        "\n",
        "LangChain provides a set of default prompt templates that can be used to generate prompts for a variety of tasks. However, there may be cases where the default prompt templates do not meet your needs. For example, you may want to create a prompt template with specific dynamic instructions for your language model. In such cases, you can create a custom prompt template.\n",
        "\n",
        "## Creating a Custom Prompt Template\n",
        "\n",
        "There are essentially two distinct prompt templates available - string prompt templates and chat prompt templates. String prompt templates provides a simple prompt in string format, while chat prompt templates produces a more structured prompt to be used with a chat API.\n",
        "\n",
        "In this guide, we will create a custom prompt using a string prompt template.\n",
        "\n",
        "To create a custom string prompt template, there are two requirements:\n",
        "\n",
        "It has an input_variables attribute that exposes what input variables the prompt template expects.\n",
        "It exposes a format method that takes in keyword arguments corresponding to the expected input_variables and returns the formatted prompt.\n",
        "We will create a custom prompt template that takes in the function name as input and formats the prompt to provide the source code of the function. To achieve this, let's first create a function that will return the source code of a function given its name."
      ],
      "metadata": {
        "id": "38zPJhfkMcpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "\n",
        "\n",
        "def get_source_code(function_name):\n",
        "    # Get the source code of the function\n",
        "    return inspect.getsource(function_name)"
      ],
      "metadata": {
        "id": "8fCzLZwBSWs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll create a custom prompt template that takes in the function name as input, and formats the prompt template to provide the source code of the function."
      ],
      "metadata": {
        "id": "IO3IadRTSXhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import StringPromptTemplate\n",
        "from pydantic import BaseModel, validator\n",
        "\n",
        "\n",
        "class FunctionExplainerPromptTemplate(StringPromptTemplate, BaseModel):\n",
        "    \"\"\"A custom prompt template that takes in the function object as input, and formats the prompt template to provide the source code of the function.\"\"\"\n",
        "\n",
        "    @validator(\"input_variables\")\n",
        "    def validate_input_variables(cls, v):\n",
        "        \"\"\"Validate that the input variables are correct.\"\"\"\n",
        "        if len(v) != 1 or \"function_object\" not in v:\n",
        "            raise ValueError(\"function_object must be the only input_variable.\")\n",
        "        return v\n",
        "\n",
        "    def format(self, **kwargs) -> str:\n",
        "        # Get the source code of the function\n",
        "        source_code = get_source_code(kwargs[\"function_object\"])\n",
        "\n",
        "        # Generate the prompt to be sent to the language model\n",
        "        prompt = f\"\"\"\n",
        "Given the function name and source code, generate an English language explanation of the function.\n",
        "Function Name: {kwargs[\"function_object\"].__name__}\n",
        "Source Code:\n",
        "{source_code}\n",
        "Explanation:\n",
        "        \"\"\"\n",
        "        return prompt\n",
        "\n",
        "    def _prompt_type(self):\n",
        "        return \"function-explainer\""
      ],
      "metadata": {
        "id": "PLKdjIJESZMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have created a custom prompt template, we can use it to generate prompts for our task."
      ],
      "metadata": {
        "id": "VN4fW6SLSkwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fn_explainer = FunctionExplainerPromptTemplate(input_variables=[\"function_object\"])\n",
        "\n",
        "# Generate a prompt for the function \"get_source_code\"\n",
        "prompt = fn_explainer.format(function_object=get_source_code)\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aM_azOvUSlrt",
        "outputId": "177aa86d-df65-4f6f-d5cb-c96ddb1e6b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Given the function name and source code, generate an English language explanation of the function.\n",
            "Function Name: get_source_code\n",
            "Source Code:\n",
            "def get_source_code(function_name):\n",
            "    # Get the source code of the function\n",
            "    return inspect.getsource(function_name)\n",
            "\n",
            "Explanation:\n",
            "        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few-shot prompt templates\n",
        "\n",
        "In this tutorial, we'll learn how to create a prompt template that uses few shot examples. A few shot prompt template can be constructed from either a set of examples, or from an Example Selector object."
      ],
      "metadata": {
        "id": "ODsFdD8oS_22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using an example set\n",
        "\n",
        "### Create the example set\n",
        "\n",
        "To get started, create a list of few shot examples. Each example should be a dictionary with the keys being the input variables and the values being the values for those input variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "yfcLVVejTLqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\n",
        "        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
        "        \"answer\":\n",
        "            \"\"\"\n",
        "            Are follow up questions needed here: Yes.\n",
        "            Follow up: How old was Muhammad Ali when he died?\n",
        "            Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
        "            Follow up: How old was Alan Turing when he died?\n",
        "            Intermediate answer: Alan Turing was 41 years old when he died.\n",
        "            So the final answer is: Muhammad Ali\n",
        "            \"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"When was the founder of craigslist born?\",\n",
        "        \"answer\":\n",
        "            \"\"\"\n",
        "            Are follow up questions needed here: Yes.\n",
        "            Follow up: Who was the founder of craigslist?\n",
        "            Intermediate answer: Craigslist was founded by Craig Newmark.\n",
        "            Follow up: When was Craig Newmark born?\n",
        "            Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
        "            So the final answer is: December 6, 1952\n",
        "            \"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Who was the maternal grandfather of George Washington?\",\n",
        "        \"answer\":\n",
        "            \"\"\"\n",
        "            Are follow up questions needed here: Yes.\n",
        "            Follow up: Who was the mother of George Washington?\n",
        "            Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
        "            Follow up: Who was the father of Mary Ball Washington?\n",
        "            Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
        "            So the final answer is: Joseph Ball\n",
        "            \"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",\n",
        "        \"answer\":\n",
        "            \"\"\"\n",
        "            Are follow up questions needed here: Yes.\n",
        "            Follow up: Who is the director of Jaws?\n",
        "            Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
        "            Follow up: Where is Steven Spielberg from?\n",
        "            Intermediate Answer: The United States.\n",
        "            Follow up: Who is the director of Casino Royale?\n",
        "            Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
        "            Follow up: Where is Martin Campbell from?\n",
        "            Intermediate Answer: New Zealand.\n",
        "            So the final answer is: No\n",
        "            \"\"\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "kiMybMA-vDm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a formatter for the few shot examples\n",
        "\n",
        "Configure a formatter that will format the few shot examples into a string. This formatter should be a PromptTemplate object."
      ],
      "metadata": {
        "id": "8O1qjnvivQ4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\")\n",
        "\n",
        "print(example_prompt.format(**examples[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55n0sAG8vV6o",
        "outputId": "7733b002-c5cf-4808-803b-5acf4e1486a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
            "\n",
            "            Are follow up questions needed here: Yes.\n",
            "            Follow up: How old was Muhammad Ali when he died?\n",
            "            Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
            "            Follow up: How old was Alan Turing when he died?\n",
            "            Intermediate answer: Alan Turing was 41 years old when he died.\n",
            "            So the final answer is: Muhammad Ali\n",
            "            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feed examples and formatter to FewShotPromptTemplate\n",
        "\n",
        "Finally, create a FewShotPromptTemplate object. This object takes in the few shot examples and the formatter for the few shot examples."
      ],
      "metadata": {
        "id": "KSlFxKkRvYI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    suffix=\"Question: {input}\",\n",
        "    input_variables=[\"input\"]\n",
        ")\n",
        "\n",
        "print(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dtk2iFTwva8L",
        "outputId": "fca14bad-3d68-4c0b-ac9d-f66aa6962bec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
            "\n",
            "            Are follow up questions needed here: Yes.\n",
            "            Follow up: How old was Muhammad Ali when he died?\n",
            "            Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
            "            Follow up: How old was Alan Turing when he died?\n",
            "            Intermediate answer: Alan Turing was 41 years old when he died.\n",
            "            So the final answer is: Muhammad Ali\n",
            "            \n",
            "\n",
            "Question: When was the founder of craigslist born?\n",
            "\n",
            "            Are follow up questions needed here: Yes.\n",
            "            Follow up: Who was the founder of craigslist?\n",
            "            Intermediate answer: Craigslist was founded by Craig Newmark.\n",
            "            Follow up: When was Craig Newmark born?\n",
            "            Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
            "            So the final answer is: December 6, 1952\n",
            "            \n",
            "\n",
            "Question: Who was the maternal grandfather of George Washington?\n",
            "\n",
            "            Are follow up questions needed here: Yes.\n",
            "            Follow up: Who was the mother of George Washington?\n",
            "            Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
            "            Follow up: Who was the father of Mary Ball Washington?\n",
            "            Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
            "            So the final answer is: Joseph Ball\n",
            "            \n",
            "\n",
            "Question: Are both the directors of Jaws and Casino Royale from the same country?\n",
            "\n",
            "            Are follow up questions needed here: Yes.\n",
            "            Follow up: Who is the director of Jaws?\n",
            "            Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
            "            Follow up: Where is Steven Spielberg from?\n",
            "            Intermediate Answer: The United States.\n",
            "            Follow up: Who is the director of Casino Royale?\n",
            "            Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
            "            Follow up: Where is Martin Campbell from?\n",
            "            Intermediate Answer: New Zealand.\n",
            "            So the final answer is: No\n",
            "            \n",
            "\n",
            "Question: Who was the father of Mary Ball Washington?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using an example selector\n",
        "\n",
        "### Feed examples into ExampleSelector\n",
        "\n",
        "We will reuse the example set and the formatter from the previous section. However, instead of feeding the examples directly into the `FewShotPromptTemplate` object, we will feed them into an `ExampleSelector` object.\n",
        "\n",
        "In this tutorial, we will use the `SemanticSimilarityExampleSelector` class. This class selects few shot examples based on their similarity to the input. It uses an embedding model to compute the similarity between the input and the few shot examples, as well as a vector store to perform the nearest neighbor search."
      ],
      "metadata": {
        "id": "DBJlV_3DvfUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "\n",
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    # This is the list of examples available to select from.\n",
        "    examples,\n",
        "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
        "    OpenAIEmbeddings(),\n",
        "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
        "    Chroma,\n",
        "    # This is the number of examples to produce.\n",
        "    k=1\n",
        ")\n",
        "\n",
        "# Select the most similar example to the input.\n",
        "question = \"Who was the father of Mary Ball Washington?\"\n",
        "selected_examples = example_selector.select_examples({\"question\": question})\n",
        "print(f\"Examples most similar to the input: {question}\")\n",
        "for example in selected_examples:\n",
        "    print(\"\\n\")\n",
        "    for k, v in example.items():\n",
        "        print(f\"{k}: {v}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zd3FmanWvrRx",
        "outputId": "f4e0bebd-3ac0-4973-dd95-393e42d85740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Examples most similar to the input: Who was the father of Mary Ball Washington?\n",
            "\n",
            "\n",
            "answer: \n",
            "            Are follow up questions needed here: Yes.\n",
            "            Follow up: Who was the mother of George Washington?\n",
            "            Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
            "            Follow up: Who was the father of Mary Ball Washington?\n",
            "            Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
            "            So the final answer is: Joseph Ball\n",
            "            \n",
            "question: Who was the maternal grandfather of George Washington?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feed example selector into FewShotPromptTemplate\n",
        "\n",
        "Finally, create a `FewShotPromptTemplate` object. This object takes in the example selector and the formatter for the few shot examples.\n",
        "\n"
      ],
      "metadata": {
        "id": "lMevAwN-wbBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    suffix=\"Question: {input}\",\n",
        "    input_variables=[\"input\"]\n",
        ")\n",
        "\n",
        "print(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDdC0AciwfNL",
        "outputId": "0d35df2f-d0ea-45cb-9937-a87527879c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Who was the maternal grandfather of George Washington?\n",
            "\n",
            "            Are follow up questions needed here: Yes.\n",
            "            Follow up: Who was the mother of George Washington?\n",
            "            Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
            "            Follow up: Who was the father of Mary Ball Washington?\n",
            "            Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
            "            So the final answer is: Joseph Ball\n",
            "            \n",
            "\n",
            "Question: Who was the father of Mary Ball Washington?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few shot examples for chat models\n",
        "\n",
        "There does not appear to be solid consensus on how best to do few shot prompting. As a result, we are not solidifying any abstractions around this yet but rather using existing abstractions.\n",
        "\n",
        "## Alternating Human/AI messages\n",
        "\n",
        "The first way of doing few shot prompting relies on using alternating human/ai messages. See an example of this below.\n",
        "\n"
      ],
      "metadata": {
        "id": "bFhLxdJDx5af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(\n",
        "    \"You are a helpful assistant that translates english to pirate\"\n",
        ")\n",
        "example_human_message_prompt = HumanMessagePromptTemplate.from_template(\"Hi\")\n",
        "example_ai_message_prompt = AIMessagePromptTemplate.from_template(\"Argh me mateys\")\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(\"{text}\")\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    system_message_prompt,\n",
        "    example_human_message_prompt,\n",
        "    example_ai_message_prompt,\n",
        "    human_message_prompt\n",
        "])\n",
        "\n",
        "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
        "\n",
        "print(chain.run(\"I love programming.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yvVDlGMx_d0",
        "outputId": "42680430-322d-43ad-a30c-6e865c8c4251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I be havin' a fondness fer programmin', me heartie.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## System Messages\n",
        "\n",
        "OpenAI provides an optional name parameter that they also recommend using in conjunction with system messages to do few shot prompting. Here is an example of how to do that below.\n",
        "\n"
      ],
      "metadata": {
        "id": "SO3_EgCmyNC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message_prompt = SystemMessagePromptTemplate.from_template(\n",
        "    \"You are a helpful assistant that translates english to pirate\"\n",
        ")\n",
        "example_human_message_prompt = HumanMessagePromptTemplate.from_template(\"Hi\", additional_kwargs={\"name\": \"example_user\"})\n",
        "example_ai_message_prompt = AIMessagePromptTemplate.from_template(\"Argh me mateys\", additional_kwargs={\"name\": \"example_assistant\"})\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(\"{text}\")\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    system_message_prompt,\n",
        "    example_human_message_prompt,\n",
        "    example_ai_message_prompt,\n",
        "    human_message_prompt\n",
        "])\n",
        "\n",
        "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
        "\n",
        "print(chain.run(\"I love programming.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nRKLsr5yXEx",
        "outputId": "e88bdfd5-21f6-4d73-fe86-1be683ff524d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I be havin' a fondness fer programmin', me heartie.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Format template output\n",
        "\n",
        "You can extract the output of a `Prompt` in three main types: string, list of messages and `ChatPromptValue`. You may use any of these outputs base on your needs."
      ],
      "metadata": {
        "id": "clzANB0Jywlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_str1 = chat_prompt.format(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n",
        "output_str2 = chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_string()\n",
        "print(\"output_str1\", output_str1)\n",
        "print(\"output_str2\", output_str2)\n",
        "\n",
        "output_prompt_value = chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n",
        "print(\"output_prompt_value\", output_prompt_value)\n",
        "\n",
        "output_messages = chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages()\n",
        "print(\"output_messages\", output_messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFUQfteazDKB",
        "outputId": "2ae37c70-7a85-4d5c-a7f2-24369e3850ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output_str1 System: You are a helpful assistant that translates english to pirate\n",
            "Human: Hi\n",
            "AI: Argh me mateys\n",
            "Human: I love programming.\n",
            "output_str2 System: You are a helpful assistant that translates english to pirate\n",
            "Human: Hi\n",
            "AI: Argh me mateys\n",
            "Human: I love programming.\n",
            "output_prompt_value messages=[SystemMessage(content='You are a helpful assistant that translates english to pirate', additional_kwargs={}), HumanMessage(content='Hi', additional_kwargs={'name': 'example_user'}, example=False), AIMessage(content='Argh me mateys', additional_kwargs={'name': 'example_assistant'}, example=False), HumanMessage(content='I love programming.', additional_kwargs={}, example=False)]\n",
            "output_messages [SystemMessage(content='You are a helpful assistant that translates english to pirate', additional_kwargs={}), HumanMessage(content='Hi', additional_kwargs={'name': 'example_user'}, example=False), AIMessage(content='Argh me mateys', additional_kwargs={'name': 'example_assistant'}, example=False), HumanMessage(content='I love programming.', additional_kwargs={}, example=False)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Types of MessagePromptTemplate\n",
        "\n",
        "LangChain provides different types of `MessagePromptTemplate`. The most commonly used are `AIMessagePromptTemplate`, `SystemMessagePromptTemplate` and `HumanMessagePromptTemplate`, which create an AI message, system message and human message respectively.\n",
        "\n",
        "However, in cases where the chat model supports taking chat message with arbitrary role, you can use `ChatMessagePromptTemplate`, which allows user to specify the role name."
      ],
      "metadata": {
        "id": "bLSo8Q244Fr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatMessagePromptTemplate\n",
        "\n",
        "chat_message_prompt = ChatMessagePromptTemplate.from_template(\n",
        "    role=\"Jedi\",\n",
        "    template=\"May the {subject} be with you\")\n",
        "\n",
        "chat_message_prompt.format(subject=\"force\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hvu1u1y_4M7c",
        "outputId": "e46d94d2-ef2a-456f-fe82-88135f9eb3f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatMessage(content='May the force be with you', additional_kwargs={}, role='Jedi')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain also provides `MessagesPlaceholder`, which gives you full control of what messages to be rendered during formatting. This can be useful when you are uncertain of what role you should be using for your message prompt templates or when you wish to insert a list of messages during formatting."
      ],
      "metadata": {
        "id": "0hq5Gc0w4aT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import MessagesPlaceholder\n",
        "\n",
        "message_placeholder = MessagesPlaceholder(variable_name=\"conversation\")\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
        "    \"Summarize our conversation so far in {word_count} words.\"\n",
        ")\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([message_placeholder, human_message_prompt])\n",
        "\n",
        "human_message = HumanMessage(content=\"What is the best way to learn programming?\")\n",
        "ai_message = AIMessage(content=\"\"\"\\\n",
        "1. Choose a programming language: Decide on a programming language that you want to learn.\n",
        "\n",
        "2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\n",
        "\n",
        "3. Practice, practice, practice: The best way to learn programming is through hands-on experience\\\n",
        "\"\"\")\n",
        "\n",
        "chat_prompt.format_prompt(conversation=[human_message, ai_message], word_count=\"10\").to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqc9WtKa4ccX",
        "outputId": "c173de31-a81d-46b1-a9b6-9d6726a58d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='What is the best way to learn programming?', additional_kwargs={}, example=False),\n",
              " AIMessage(content='1. Choose a programming language: Decide on a programming language that you want to learn.\\n\\n2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\\n\\n3. Practice, practice, practice: The best way to learn programming is through hands-on experience', additional_kwargs={}, example=False),\n",
              " HumanMessage(content='Summarize our conversation so far in 10 words.', additional_kwargs={}, example=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partial prompt templates\n",
        "\n",
        "Like other methods, it can make sense to \"partial\" a prompt template - eg pass in a subset of the required values, as to create a new prompt template which expects only the remaining subset of values.\n",
        "\n",
        "LangChain supports this in two ways:\n",
        "\n",
        "Partial formatting with string values.\n",
        "Partial formatting with functions that return string values.\n",
        "These two different ways support different use cases. In the examples below, we go over the motivations for both use cases as well as how to do it in LangChain."
      ],
      "metadata": {
        "id": "M3CPElkWXsaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partials with Strings\n",
        "\n",
        "One common use case for wanting to partial a prompt template is if you get some of the variables before others. For example, suppose you have a prompt template that requires two variables, foo and baz. If you get the foo value early on in the chain, but the baz value later, it can be annoying to wait until you have both variables in the same place to pass them to the prompt template. Instead, you can partial the prompt template with the foo value, and then pass the partialed prompt template along and just use that. Below is an example of doing this:"
      ],
      "metadata": {
        "id": "98akCFs2Xwvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(template=\"{foo} {bar}\", input_variables=[\"foo\", \"bar\"])\n",
        "\n",
        "# partial prompt\n",
        "partial_prompt = prompt.partial(foo=\"Hello\")\n",
        "print(partial_prompt)\n",
        "\n",
        "# full message\n",
        "message = partial_prompt.format(bar=\"World!\")\n",
        "print(message)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6_p5dVhX725",
        "outputId": "4dc0d46b-673f-4f16-dad2-09185287b8a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['bar'] output_parser=None partial_variables={'foo': 'Hello'} template='{foo} {bar}' template_format='f-string' validate_template=True\n",
            "Hello World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also just initialize the prompt with the partialed variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "mx0WrSn9YegF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(template=\"{foo} {bar}\", input_variables=[\"bar\"], partial_variables={\"foo\": \"Hello\"})\n",
        "print(prompt.format(bar=\"baz\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0D8-PkFYfHu",
        "outputId": "995aa394-2c68-4d6d-a9c0-099992c14bc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello baz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partial With Functions\n",
        "\n",
        "The other common use is to partial with a function. The use case for this is when you have a variable you know that you always want to fetch in a common way. A prime example of this is with date or time. Imagine you have a prompt which you always want to have the current date. You can't hard code it in the prompt, and passing it along with the other input variables is a bit annoying. In this case, it's very handy to be able to partial the prompt with a function that always returns the current date."
      ],
      "metadata": {
        "id": "qbtHjCYZYkQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def get_datetime():\n",
        "  now = datetime.now()\n",
        "  return now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Tell me a {adjective} joke about the day {date}\",\n",
        "    input_variables=[\"adjective\", \"date\"]\n",
        ")\n",
        "partial_prompt = prompt.partial(date=get_datetime)\n",
        "print(partial_prompt.format(adjective=\"funny\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAMYYGOBYu5A",
        "outputId": "87455e08-3933-48f9-f44a-7e98ac55385f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell me a funny joke about the day 07/20/2023, 17:43:02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also just initialize the prompt with the partialed variables, which often makes more sense in this workflow.\n",
        "\n"
      ],
      "metadata": {
        "id": "tlO9R2IRZhqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"Tell me a {adjective} joke about day {date}\",\n",
        "    input_variables=[\"adjective\"],\n",
        "    partial_variables={\"date\": get_datetime}\n",
        ")\n",
        "print(prompt.format(adjective=\"sad\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jz61hSvZik5",
        "outputId": "6d15db59-63d7-4028-ad34-c1a4c7fa0221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell me a sad joke about day 07/20/2023, 17:44:44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Composition\n",
        "\n",
        "This section goes over how to compose multiple prompts together. This can be useful when you want to reuse parts of prompts. This can be done with a PipelinePrompt. A PipelinePrompt consists of two main parts:\n",
        "\n",
        "* *Final prompt*: This is the final prompt that is returned\n",
        "* *Pipeline prompts*: This is a list of tuples, consisting of a string name and a prompt template. Each prompt template will be formatted and then passed to future prompt templates as a variable with the same name."
      ],
      "metadata": {
        "id": "Mq-Jn8iFaEqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "full_template = \"\"\"{introduction}\n",
        "\n",
        "{example}\n",
        "\n",
        "{start}\"\"\"\n",
        "full_prompt = PromptTemplate.from_template(full_template)\n",
        "\n",
        "introduction_template = \"You are imporsotating {person}.\"\n",
        "introduction_prompt = PromptTemplate.from_template(introduction_template)\n",
        "\n",
        "example_template = \"\"\"Here's an example of an interaction:\n",
        "\n",
        "Q: {example_q}\n",
        "A: {example_a}\"\"\"\n",
        "example_prompt = PromptTemplate.from_template(example_template)\n",
        "\n",
        "start_template = \"\"\"Now, do this for real!\n",
        "\n",
        "Q: {input}\n",
        "A:\"\"\"\n",
        "start_prompt = PromptTemplate.from_template(start_template)\n",
        "\n",
        "input_prompts = [\n",
        "    (\"introduction\", introduction_prompt),\n",
        "    (\"example\", example_prompt),\n",
        "    (\"start\", start_prompt)\n",
        "]\n",
        "pipeline_prompt = PipelinePromptTemplate(\n",
        "    final_prompt=full_prompt,\n",
        "    pipeline_prompts=input_prompts\n",
        ")\n",
        "\n",
        "print(pipeline_prompt.input_variables)\n",
        "\n",
        "print(pipeline_prompt.format(\n",
        "    person=\"Elon Musk\",\n",
        "    example_q=\"What's your favorite car?\",\n",
        "    example_a=\"Tesla\",\n",
        "    input=\"What's your favorite social media site?\"\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ymQwE-OaLp-",
        "outputId": "613f158a-84bc-4b63-a919-5b449833359a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['input', 'person', 'example_q', 'example_a']\n",
            "You are imporsotating Elon Musk.\n",
            "\n",
            "Here's an example of an interaction:\n",
            "\n",
            "Q: What's your favorite car?\n",
            "A: Tesla\n",
            "\n",
            "Now, do this for real!\n",
            "\n",
            "Q: What's your favorite social media site?\n",
            "A:\n"
          ]
        }
      ]
    }
  ]
}