{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8LvCa1P3WUwW9AW6Ir5nc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoseinBahmany/learning-llms/blob/main/langchain/03_example_selectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai chromadb tiktoken numpy faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQcUHgvgNpHz",
        "outputId": "c2d23e7f-c5ec-455d-dd6c-78d7e406c565"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.238)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.18)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.5.13)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.12)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.11)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.5.3)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.1)\n",
            "Requirement already satisfied: fastapi<0.100.0,>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.99.1)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.23.1)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.7.1)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.2.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.15.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.13.3)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.3.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi<0.100.0,>=0.95.2->chromadb) (0.27.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.11.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.4)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (3.7.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.1.2)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"1516792b8aa8d598271fd69823f3590da610d429c776fff1deca86f4415bc818\""
      ],
      "metadata": {
        "id": "tlZ-aY3hNqEX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have a large number of examples, you may need to select which ones to include in the prompt. The Example Selector is the class responsible for doing so.\n",
        "\n",
        "The base interface is defined as below:\n",
        "\n"
      ],
      "metadata": {
        "id": "kCzm9_YaM9gI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoCFdVqvMz4S"
      },
      "outputs": [],
      "source": [
        "class BaseExampleSelector(ABC):\n",
        "    \"\"\"Interface for selecting examples to include in prompts.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\n",
        "        \"\"\"Select which examples to use based on the inputs.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The only method it needs to expose is a `select_examples` method. This takes in the input variables and then returns a list of examples. It is up to each specific implementation as to how those examples are selected. Let's take a look at some below.\n",
        "\n"
      ],
      "metadata": {
        "id": "4yrp2_gnNDzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom example selector\n",
        "\n",
        "To illustrate how to create a custom example selector, we'll create example selector that selects 2 random examples from a given list of examples.\n",
        "\n",
        "An `ExampleSelector` must implement two methods:\n",
        " * An `add_example` method which takes in an example and adds it into the ExampleSelector\n",
        " * A `select_examples` method which takes in input variables (which are meant to be user input) and returns a list of examples to use in the few shot prompt.\n",
        "Let's implement a custom `ExampleSelector` that just selects two examples at random."
      ],
      "metadata": {
        "id": "vNdHjdXoNHhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
        "from typing import Dict, List\n",
        "import numpy as np\n",
        "\n",
        "class CustomExampleSelector(BaseExampleSelector):\n",
        "  def __init__(self, examples: List[Dict[str, str]]):\n",
        "    self.examples = examples\n",
        "\n",
        "  def add_example(self, example: Dict[str, str]) -> None:\n",
        "    \"\"\"Add a new example to the store.\"\"\"\n",
        "    self.examples.append(example)\n",
        "\n",
        "  def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\n",
        "    \"\"\"Select 2 examples by random\"\"\"\n",
        "    return np.random.choice(self.examples, size=2, replace=False)\n",
        "\n",
        "examples = [\n",
        "    {\"foo\": 1},\n",
        "    {\"bar\": 2},\n",
        "    {\"baz\": 3}\n",
        "]\n",
        "\n",
        "# Initialize example selector\n",
        "example_selector = CustomExampleSelector(examples)\n",
        "\n",
        "# Select examples\n",
        "print(example_selector.select_examples({\"foo\": \"foo\"}))\n",
        "\n",
        "# Add new examples to the selctor\n",
        "example_selector.add_example({\"qux\": 4})\n",
        "print(example_selector.examples)\n",
        "\n",
        "# Select examples\n",
        "print(example_selector.select_examples({\"foo\": \"foo\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zly68NENvH4",
        "outputId": "a2842205-8adb-4619-e57d-d09464e0b628"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'foo': 1} {'bar': 2}]\n",
            "[{'foo': 1}, {'bar': 2}, {'baz': 3}, {'qux': 4}]\n",
            "[{'foo': 1} {'baz': 3}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select by length\n",
        "\n",
        "\n",
        "The `LengthBasedExampleSelector` selects which examples to use based on length. This is useful when you are worried about constructing a prompt that will go over the length of the context window. For longer inputs, it will select fewer examples to include, while for shorter inputs it will select more."
      ],
      "metadata": {
        "id": "ocJqb2wuPjK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "\n",
        "examples = [\n",
        "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
        "    {\"input\": \"tall\", \"output\": \"short\"},\n",
        "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
        "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
        "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
        "]\n",
        "\n",
        "example_prompt = PromptTemplate.from_template(\n",
        "    \"Input: {input}\\nOutput: {output}\"\n",
        ")\n",
        "\n",
        "example_selector = LengthBasedExampleSelector(\n",
        "    # These are the examples it has available to choose from.\n",
        "    examples=examples,\n",
        "    # This is the PromptTemplate being used to format the examples.\n",
        "    example_prompt=example_prompt,\n",
        "    # This is the maximum length that the formatted examples should be.\n",
        "    # Length is measured by the get_text_length function below.\n",
        "    max_length=25,\n",
        "    # This is the function used to get the length of a string, which is used\n",
        "    # to determine which examples to include. It is commented out because\n",
        "    # it is provided as a default value if none is specified.\n",
        "    # get_text_length: Callable[[str], int] = lambda x: len(re.split(\"\\n| \", x))\n",
        ")\n",
        "\n",
        "prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Give the antonym of every input\",\n",
        "    suffix=\"Input {adjective}\\nOutput:\",\n",
        "    input_variables=[\"adjective\"]\n",
        ")"
      ],
      "metadata": {
        "id": "fIlkwnHkPwiU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt.format(adjective=\"big\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kevVp1mJRQZs",
        "outputId": "a5613fcd-df5f-4b2e-ad38-f6537841a41c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the antonym of every input\n",
            "\n",
            "Input: happy\n",
            "Output: sad\n",
            "\n",
            "Input: tall\n",
            "Output: short\n",
            "\n",
            "Input: energetic\n",
            "Output: lethargic\n",
            "\n",
            "Input: sunny\n",
            "Output: gloomy\n",
            "\n",
            "Input: windy\n",
            "Output: calm\n",
            "\n",
            "Input big\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# An example with long input, so it selects only one example.\n",
        "long_string = \"big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\"\n",
        "print(prompt.format(adjective=long_string))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk1JipYCRavc",
        "outputId": "fdd07762-a3ba-42f0-c8cd-3422da40d0b3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the antonym of every input\n",
            "\n",
            "Input: happy\n",
            "Output: sad\n",
            "\n",
            "Input big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can add an example to an example selector as well.\n",
        "new_example = {\"input\": \"big\", \"output\": \"small\"}\n",
        "prompt.example_selector.add_example(new_example)\n",
        "print(prompt.format(adjective=\"enthusiastic\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMKGi6_5RiGd",
        "outputId": "fd8b4f80-0547-4cdf-ff3c-dc5d955ef4eb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the antonym of every input\n",
            "\n",
            "Input: happy\n",
            "Output: sad\n",
            "\n",
            "Input: tall\n",
            "Output: short\n",
            "\n",
            "Input: energetic\n",
            "Output: lethargic\n",
            "\n",
            "Input: sunny\n",
            "Output: gloomy\n",
            "\n",
            "Input: windy\n",
            "Output: calm\n",
            "\n",
            "Input: big\n",
            "Output: small\n",
            "\n",
            "Input enthusiastic\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select by maximal marginal relevance (MMR)\n",
        "\n",
        "The `MaxMarginalRelevanceExampleSelector` selects examples based on a combination of which examples are most similar to the inputs, while also optimizing for diversity. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs, and then iteratively adding them while penalizing them for closeness to already selected examples."
      ],
      "metadata": {
        "id": "lQyz4eA-R-Gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector import (\n",
        "    MaxMarginalRelevanceExampleSelector,\n",
        "    SemanticSimilarityExampleSelector\n",
        ")\n",
        "from langchain.vectorstores import Chroma, FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
        "    {\"input\": \"tall\", \"output\": \"short\"},\n",
        "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
        "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
        "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
        "]\n",
        "\n",
        "example_prompt = PromptTemplate.from_template(\n",
        "    \"Input: {input}\\nOutput: {output}\"\n",
        ")\n",
        "\n",
        "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
        "    # This is the list of examples available to select from.\n",
        "    examples,\n",
        "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
        "    OpenAIEmbeddings(),\n",
        "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
        "    FAISS,\n",
        "    # This is the number of examples to produce.\n",
        "    k=5,\n",
        ")\n",
        "\n",
        "mmr_prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Give the antonym of every input\",\n",
        "    suffix=\"Input: {adjective}\\nOutput:\",\n",
        "    input_variables=[\"adjective\"]\n",
        ")\n",
        "\n",
        "# Input is a feeling, so should select the happy/sad example as the first one\n",
        "print(mmr_prompt.format(adjective=\"worried\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiPBxBRmSILd",
        "outputId": "0cc6717c-5ba6-48b3-cebb-e57f3ef6b4d6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the antonym of every input\n",
            "\n",
            "Input: happy\n",
            "Output: sad\n",
            "\n",
            "Input: windy\n",
            "Output: calm\n",
            "\n",
            "Input: energetic\n",
            "Output: lethargic\n",
            "\n",
            "Input: tall\n",
            "Output: short\n",
            "\n",
            "Input: sunny\n",
            "Output: gloomy\n",
            "\n",
            "Input: worried\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare this to what we would just get if we went solely off of similarity, by using `SemanticSimilarityExampleSelector` instead of `MaxMarginalRelevanceExampleSelector`."
      ],
      "metadata": {
        "id": "EuCX-T-9Tohv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    # This is the list of examples available to select from.\n",
        "    examples,\n",
        "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
        "    OpenAIEmbeddings(),\n",
        "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
        "    FAISS,\n",
        "    # This is the number of examples to produce.\n",
        "    k=5,\n",
        ")\n",
        "\n",
        "mmr_prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Give the antonym of every input\",\n",
        "    suffix=\"Input: {adjective}\\nOutput:\",\n",
        "    input_variables=[\"adjective\"]\n",
        ")\n",
        "\n",
        "# Input is a feeling, so should select the happy/sad example as the first one\n",
        "print(mmr_prompt.format(adjective=\"worried\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKbiplP2Tn8W",
        "outputId": "b24eb3cf-17f1-4ca0-d366-4acb3d1d8464"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the antonym of every input\n",
            "\n",
            "Input: happy\n",
            "Output: sad\n",
            "\n",
            "Input: sunny\n",
            "Output: gloomy\n",
            "\n",
            "Input: windy\n",
            "Output: calm\n",
            "\n",
            "Input: energetic\n",
            "Output: lethargic\n",
            "\n",
            "Input: tall\n",
            "Output: short\n",
            "\n",
            "Input: worried\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select by n-gram overlap\n",
        "\n",
        "The `NGramOverlapExampleSelector` selects and orders examples based on which examples are most similar to the input, according to an ngram overlap score. The ngram overlap score is a float between 0.0 and 1.0, inclusive.\n",
        "\n",
        "The selector allows for a threshold score to be set. Examples with an ngram overlap score less than or equal to the threshold are excluded. The threshold is set to -1.0, by default, so will not exclude any examples, only reorder them. Setting the threshold to 0.0 will exclude examples that have no ngram overlaps with the input."
      ],
      "metadata": {
        "id": "X-bXxO_7Xg96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
        "from langchain.prompts.example_selector import NGramOverlapExampleSelector\n",
        "\n",
        "# These are examples of a fictional translation task.\n",
        "examples = [\n",
        "    {\"input\": \"See Spot run.\", \"output\": \"Ver correr a Spot.\"},\n",
        "    {\"input\": \"My dog barks.\", \"output\": \"Mi perro ladra.\"},\n",
        "    {\"input\": \"Spot can run.\", \"output\": \"Spot puede correr.\"},\n",
        "]\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Input: {input}\\nOutput: {output}\",\n",
        ")\n",
        "\n",
        "example_selector = NGramOverlapExampleSelector(\n",
        "    # These are the examples it has available to choose from.\n",
        "    examples=examples,\n",
        "    # This is the PromptTemplate being used to format the examples.\n",
        "    example_prompt=example_prompt,\n",
        "    # This is the threshold, at which selector stops.\n",
        "    # It is set to -1.0 by default.\n",
        "    threshold=-1.0,\n",
        "    # For negative threshold:\n",
        "    # Selector sorts examples by ngram overlap score, and excludes none.\n",
        "    # For threshold greater than 1.0:\n",
        "    # Selector excludes all examples, and returns an empty list.\n",
        "    # For threshold equal to 0.0:\n",
        "    # Selector sorts examples by ngram overlap score,\n",
        "    # and excludes those with no ngram overlap with input.\n",
        ")\n",
        "\n",
        "prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Give the Spanish tranlation of every input\",\n",
        "    suffix=\"Input: {sentence}\\nOutput:\",\n",
        "    input_variables=[\"sentence\"]\n",
        ")\n",
        "\n",
        "# An example input with large ngram overlap with \"Spot can run.\" and no overlap with \"My dog barks.\"\n",
        "print(prompt.format(sentence=\"Spot can run fast.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_w9IF36fXpEO",
        "outputId": "afebd709-3473-4f16-f999-ba6168f919d5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the Spanish tranlation of every input\n",
            "\n",
            "Input: Spot can run.\n",
            "Output: Spot puede correr.\n",
            "\n",
            "Input: See Spot run.\n",
            "Output: Ver correr a Spot.\n",
            "\n",
            "Input: My dog barks.\n",
            "Output: Mi perro ladra.\n",
            "\n",
            "Input: Spot can run fast.\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can set a threshold at which examples are excluded.\n",
        "# For example, setting threshold equal to 0.0\n",
        "# excludes examples with no ngram overlaps with input.\n",
        "# Since \"My dog barks.\" has no ngram overlaps with \"Spot can run fast.\"\n",
        "# it is excluded.\n",
        "example_selector.threshold = 0.0\n",
        "print(prompt.format(sentence=\"Spot can run fast.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKhMTYUkYxWr",
        "outputId": "437d78b0-1ed2-4e10-db9c-98b662c263d2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the Spanish tranlation of every input\n",
            "\n",
            "Input: Spot can run.\n",
            "Output: Spot puede correr.\n",
            "\n",
            "Input: See Spot run.\n",
            "Output: Ver correr a Spot.\n",
            "\n",
            "Input: Spot can run fast.\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting small nonzero threshold\n",
        "example_selector.threshold = 0.09\n",
        "print(prompt.format(sentence=\"Spot can play fetch.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeTOgDFdY2BT",
        "outputId": "0252da06-e0a5-428f-b5d9-2d280a1e1159"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the Spanish tranlation of every input\n",
            "\n",
            "Input: Spot can run.\n",
            "Output: Spot puede correr.\n",
            "\n",
            "Input: Spot can play fetch.\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting threshold greater than 1.0\n",
        "example_selector.threshold = 1.0 + 1e-9\n",
        "print(prompt.format(sentence=\"Spot can play fetch.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J9hpv_xY5bs",
        "outputId": "be293dbc-d9cd-44c5-f984-8cec72e421ea"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the Spanish tranlation of every input\n",
            "\n",
            "Input: Spot can play fetch.\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select by Similarity\n",
        "\n",
        "`SemanticSimilarityExampleSelector` elects examples based on similarity to the inputs. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs."
      ],
      "metadata": {
        "id": "7kk_Dow2Y--O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
        "    {\"input\": \"tall\", \"output\": \"short\"},\n",
        "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
        "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
        "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
        "]\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Input: {input}\\nOutput: {output}\",\n",
        ")\n",
        "\n",
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    # This is the list of examples available to select from.\n",
        "    examples,\n",
        "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
        "    OpenAIEmbeddings(),\n",
        "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
        "    FAISS,\n",
        "    # This is the number of examples to produce.\n",
        "    k=1\n",
        ")\n",
        "\n",
        "prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Give the antonym of every input\",\n",
        "    suffix=\"Input: {adjective}\\nOutput:\",\n",
        "    input_variables=[\"adjective\"],\n",
        ")\n",
        "\n",
        "# Input is a feeling, so should select the happy/sad example\n",
        "print(prompt.format(adjective=\"worried\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKY90rwIZFsI",
        "outputId": "073cd054-f6db-4b05-fbfa-024f3a50e644"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the antonym of every input\n",
            "\n",
            "Input: happy\n",
            "Output: sad\n",
            "\n",
            "Input: worried\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can add new examples to the SemanticSimilarityExampleSelector as well\n",
        "prompt.example_selector.add_example({\"input\": \"enthusiastic\", \"output\": \"apathetic\"})\n",
        "print(prompt.format(adjective=\"joyful\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDvbbCe6ZeHt",
        "outputId": "a36e1f1b-01eb-4136-a1dc-3172d221697e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the antonym of every input\n",
            "\n",
            "Input: happy\n",
            "Output: sad\n",
            "\n",
            "Input: joyful\n",
            "Output:\n"
          ]
        }
      ]
    }
  ]
}