{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4THsm94JlKiBUe/MXYXSB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoseinBahmany/learning-llms/blob/main/langchain/09_llm_chains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SnFyI6P3rDD"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai chromadb tiktoken numpy faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"1516792b8aa8d598271fd69823f3590da610d429c776fff1deca86f4415bc818\""
      ],
      "metadata": {
        "id": "-pPj6Gvh3tnb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components.\n",
        "\n",
        "LangChain provides the Chain interface for such \"chained\" applications. We define a Chain very generically as a sequence of calls to components, which can include other chains. The base interface is simple:\n",
        "\n",
        "```python\n",
        "class Chain(BaseModel, ABC):\n",
        "    \"\"\"Base interface that all chains should implement.\"\"\"\n",
        "\n",
        "    memory: BaseMemory\n",
        "    callbacks: Callbacks\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        inputs: Any,\n",
        "        return_only_outputs: bool = False,\n",
        "        callbacks: Callbacks = None,\n",
        "    ) -> Dict[str, Any]:\n",
        "        ...\n",
        "```\n",
        "\n",
        "This idea of composing components together in a chain is simple but powerful. It drastically simplifies and makes more modular the implementation of complex applications, which in turn makes it much easier to debug, maintain, and improve your applications.\n",
        "\n",
        "Chains allow us to combine multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. We can build more complex chains by combining multiple chains together, or by combining chains with other components.\n",
        "\n",
        "You can easily integrate a Chain object as a Tool in your Agent via its run method."
      ],
      "metadata": {
        "id": "Yyw8hjKE3tDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLMChain\n",
        "\n",
        "The LLMChain is most basic building block chain. It takes in a prompt template, formats it with the user input and returns the response from an LLM.\n",
        "\n",
        "To use the LLMChain, first create a prompt template."
      ],
      "metadata": {
        "id": "E0t8IBjY4Ua1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"What is a good name for a company that makes {product}?\"\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "print(chain.run(\"colorful socks\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlHYETJE4bdV",
        "outputId": "d563e404-8e72-4510-b6c9-24197c4ad399"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Socktastic!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If there are multiple variables, you can input them all at once using a dictionary."
      ],
      "metadata": {
        "id": "Ou-RQTk749g9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate.from_template(\n",
        "    \"What is a good name for {company} that makes {product}?\"\n",
        ")\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "print(chain.run({\n",
        "    \"company\": \"ABC Startup\",\n",
        "    \"product\": \"colorful socks\"\n",
        "}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpCeIQSw4_F3",
        "outputId": "ed7e95ef-ce29-4061-d5ed-22079a7aa140"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Socktastic!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use a chat model in an `LLMChain` as well:"
      ],
      "metadata": {
        "id": "pmHUdOOIX96i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
        "    \"What is a good name for a company that makes {product}?\"\n",
        ")\n",
        "chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n",
        "\n",
        "chat = ChatOpenAI(temperature=0.9)\n",
        "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
        "\n",
        "print(chain.run(\"colorful socks\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsirzqivYAND",
        "outputId": "47c0f287-12d7-48a3-ca91-ffc1ba191ca5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rainbow Threads\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Different Call Methods\n",
        "\n",
        "All classes inherited from `Chain` offer a few ways of running chain logic. The most direct one is by using `__call__` and `run`:"
      ],
      "metadata": {
        "id": "8cNJ077CYyo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0)\n",
        "prompt = PromptTemplate.from_template(\"Tell me a {adjective} joke\")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "print(chain({\"adjective\": \"corny\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUYuGKQFY47C",
        "outputId": "21e63414-e40b-4144-fa7b-d61abfa368b6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'adjective': 'corny', 'text': '\\n\\nQ: What did the fish say when it hit the wall?\\nA: Dam!'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, __call__ returns both the input and output key values. You can configure it to only return output key values by setting return_only_outputs to True."
      ],
      "metadata": {
        "id": "VJbdmw4iZWdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain({\"adjective\": \"corny\"}, return_only_outputs=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ0OOpr3ZWvt",
        "outputId": "d206acb3-a7ec-4458-84d8-8b8112ad9b87"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': '\\n\\nQ: What did the fish say when it hit the wall?\\nA: Dam!'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the Chain only outputs one output key (i.e. only has one element in its `output_keys`), you can use run method. Note that run outputs a string instead of a dictionary."
      ],
      "metadata": {
        "id": "P6NT33nHZkHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run({\"adjective\": \"corny\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KKq5JsZdZmqK",
        "outputId": "4dbb7d24-7244-400f-a89c-3dbd46901065"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nQ: What did the fish say when it hit the wall?\\nA: Dam!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case of one input key, you can input the string directly without specifying the input mapping."
      ],
      "metadata": {
        "id": "LE1bz-LfZr0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"corny\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "z04D6g_2Zs4f",
        "outputId": "32492aca-817f-4aa1-8f75-4f512e36e636"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nQ: What did the fish say when it hit the wall?\\nA: Dam!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`apply` allows you run the chain against a list of inputs:"
      ],
      "metadata": {
        "id": "vlJzd9F1ZC43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, OpenAI, LLMChain\n",
        "\n",
        "prompt_template = \"What is a good name for a company that makes {product}?\"\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=PromptTemplate.from_template(prompt_template)\n",
        ")\n",
        "\n",
        "input_list = [\n",
        "    {\"product\": \"socks\"},\n",
        "    {\"product\": \"computer\"},\n",
        "    {\"product\": \"shoes\"}\n",
        "]\n",
        "\n",
        "llm_chain.apply(input_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8tB8N0HZGAe",
        "outputId": "326f6973-a8ce-4954-800f-d1579a463a41"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': '\\n\\nSocktastic!'},\n",
              " {'text': '\\n\\nTechCore Solutions.'},\n",
              " {'text': '\\n\\nFootwear Factory.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`generate` is similar to `apply`, except it return an `LLMResult` instead of string. LLMResult often contains useful generation such as token usages and finish reason."
      ],
      "metadata": {
        "id": "gbOgKzb4ZfTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.generate(input_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wao3LO-MZHA-",
        "outputId": "26136985-522f-48ca-c3eb-4fed5e373ab9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='\\n\\nSocktastic!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nTechCore Solutions.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nFootwear Factory.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 55, 'prompt_tokens': 36, 'completion_tokens': 19}, 'model_name': 'text-davinci-003'}, run=[RunInfo(run_id=UUID('df397158-c0e2-4b9d-8165-534df99295a5')), RunInfo(run_id=UUID('83756a0a-38cb-48b8-be8d-0cb4b624c03d')), RunInfo(run_id=UUID('83a27fa3-86b7-4480-b68d-cf6ea473825d'))])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`predict` is similar to `run` method except that the input keys are specified as keyword arguments instead of a Python dict."
      ],
      "metadata": {
        "id": "TxdiQWI5Zn92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(product=\"colorful socks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "r4fUE8O8ZrJK",
        "outputId": "78e8ebbe-6847-43dd-f300-062669ead8b2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nSocktastic!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing the Outputs\n",
        "\n",
        "By default, `LLMChain` does not parse the output even if the underlying prompt object has an output parser. If you would like to apply that output parser on the LLM output, use `predict_and_parse` instead of `predict` and `apply_and_parse` instead of `apply`."
      ],
      "metadata": {
        "id": "jZY1KUbaZ4Ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "prompt = PromptTemplate(\n",
        "    template=\"List all the colors in a rainbow\",\n",
        "    input_variables=[],\n",
        "    output_parser=output_parser\n",
        ")\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "print(\"predict: \", llm_chain.predict())\n",
        "\n",
        "print(\"predict_and_parser: \", llm_chain.predict_and_parse())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqnu7ewzaBGK",
        "outputId": "97d2b8e6-e82a-42f4-b0c5-d8a4c10a6b7e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predict:  \n",
            "\n",
            "Red, orange, yellow, green, blue, indigo, violet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predict_and_parser:  ['Red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Chain\n",
        "\n",
        "To implement your own custom chain you can subclass Chain and implement the following methods:"
      ],
      "metadata": {
        "id": "-vqkTmHMZ4sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from langchain.schema import BaseLanguageModel\n",
        "from langchain.callbacks.manager import AsyncCallbackManagerForChainRun, CallbackManagerForChainRun\n",
        "from langchain.chains.base import Chain\n",
        "from langchain.prompts.base import BasePromptTemplate\n",
        "\n",
        "from typing import Any, Dict, List, Optional\n",
        "from pydantic import Extra\n",
        "\n",
        "class CustomChain(Chain):\n",
        "  \"\"\"\n",
        "  An example of a custom chain.\n",
        "  \"\"\"\n",
        "\n",
        "  prompt: BasePromptTemplate\n",
        "  llm: BaseLanguageModel\n",
        "  output_key: str = \"text\"\n",
        "\n",
        "  class Config:\n",
        "    \"\"\"Configuration for this pydantic object.\"\"\"\n",
        "\n",
        "    extra = Extra.forbid\n",
        "    arbitrary_types_allowed = True\n",
        "\n",
        "  @property\n",
        "  def input_keys(self) -> List[str]:\n",
        "    \"\"\"Will be whatever keys the prompt expects.\"\"\"\n",
        "    return self.prompt.input_variables\n",
        "\n",
        "  @property\n",
        "  def output_keys(self) -> List[str]:\n",
        "    \"\"\"Will always return text key.\"\"\"\n",
        "    return [self.output_key]\n",
        "\n",
        "  def _call(\n",
        "      self,\n",
        "      inputs: Dict[str, Any],\n",
        "      run_manager: Optional[CallbackManagerForChainRun] = None,\n",
        "  ) -> Dict[str, str]:\n",
        "    # Your custom chain logic goes here\n",
        "    # This is just an example that mimics LLMChain\n",
        "    prompt_value = self.prompt.format_prompt(**inputs)\n",
        "\n",
        "    # Whenever you call a language model, or another chain, you should pass\n",
        "    # a callback manager to it. This allows the inner run to be tracked by\n",
        "    # any callbacks that are registered on the outer run.\n",
        "    # You can always obtain a callback manager for this by calling\n",
        "    # `run_manager.get_child()` as shown below.\n",
        "    response = self.llm.generate_prompt(\n",
        "        [prompt_value],\n",
        "        callbacks=run_manager.get_child() if run_manager else None\n",
        "    )\n",
        "\n",
        "    # If you want to log something about this run, you can do so by calling\n",
        "    # methods on the `run_manager`, as shown below. This will trigger any\n",
        "    # callbacks that are registered for that event.\n",
        "    if run_manager:\n",
        "        run_manager.on_text(\"Log something about this run\")\n",
        "\n",
        "    return {self.output_key: response.generations[0][0].text}\n",
        "\n",
        "  async def _acall(\n",
        "      self,\n",
        "      inputs: Dict[str, Any],\n",
        "      run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
        "  ) -> Dict[str, str]:\n",
        "      # Your custom chain logic goes here\n",
        "      # This is just an example that mimics LLMChain\n",
        "      prompt_value = self.prompt.format_prompt(**inputs)\n",
        "\n",
        "      # Whenever you call a language model, or another chain, you should pass\n",
        "      # a callback manager to it. This allows the inner run to be tracked by\n",
        "      # any callbacks that are registered on the outer run.\n",
        "      # You can always obtain a callback manager for this by calling\n",
        "      # `run_manager.get_child()` as shown below.\n",
        "      response = await self.llm.agenerate_prompt(\n",
        "          [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
        "      )\n",
        "\n",
        "      # If you want to log something about this run, you can do so by calling\n",
        "      # methods on the `run_manager`, as shown below. This will trigger any\n",
        "      # callbacks that are registered for that event.\n",
        "      if run_manager:\n",
        "          await run_manager.on_text(\"Log something about this run\")\n",
        "\n",
        "      return {self.output_key: response.generations[0][0].text}\n",
        "\n",
        "  @property\n",
        "  def _chain_type(self) -> str:\n",
        "      return \"my_custom_chain\""
      ],
      "metadata": {
        "id": "XHs7p0vzZ8Ai"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks.stdout import StdOutCallbackHandler\n",
        "from langchain.chat_models.openai import ChatOpenAI\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "chain = CustomChain(\n",
        "    prompt=PromptTemplate.from_template(\"tell us a joke about {topic}\"),\n",
        "    llm=OpenAI(temperature=0)\n",
        ")\n",
        "\n",
        "chain.run(\"callbacks\", callbacks=[StdOutCallbackHandler()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "fLGlhb4ecUwz",
        "outputId": "2060c78d-8af9-4ad7-f6e7-47ec2d1c7682"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new CustomChain chain...\u001b[0m\n",
            "Log something about this run\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nQ: What did the asynchronous function say to the promise?\\nA: \"Don\\'t call me, I\\'ll call you!\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Memory (State)\n",
        "\n",
        "Chains can be initialized with a Memory object, which will persist data across calls to the chain. This makes a Chain stateful.\n",
        "\n",
        "Essentially, `BaseMemory` defines an interface of how langchain stores memory. It allows reading of stored data through `load_memory_variables` method and storing new data through `save_context` method."
      ],
      "metadata": {
        "id": "Tca2bctXdq3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=ChatOpenAI(temperature=0),\n",
        "    memory=ConversationBufferMemory()\n",
        ")\n",
        "\n",
        "print(conversation.run(\"Answer briefly. What are the first 3 colors of a rainbow?\"))\n",
        "print(conversation.run(\"And the next 4?\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cg9GX1Sd3Gv",
        "outputId": "070949a2-5704-4840-9c4f-1f9fbc35db9e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first three colors of a rainbow are red, orange, and yellow.\n",
            "The next four colors of a rainbow are green, blue, indigo, and violet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Router Chain\n",
        "\n",
        "This notebook demonstrates how to use the `RouterChain` paradigm to create a chain that dynamically selects the next chain to use for a given input.\n",
        "\n",
        "Router chains are made up of two components:\n",
        "\n",
        "* The `RouterChain` itself (responsible for selecting the next chain to call)\n",
        "* `destination_chains`: chains that the router chain can route to\n",
        "\n",
        "In this notebook we will focus on the different types of routing chains. We will show these routing chains used in a MultiPromptChain to create a question-answering chain that selects the prompt which is most relevant for a given question, and then answers the question using that prompt."
      ],
      "metadata": {
        "id": "R1agRHbtak8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import MultiPromptChain, LLMChain, ConversationChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
        "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
        "You are so good because you are able to break down hard problems into their component parts, \\\n",
        "answer the component parts, and then put them together to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\",\n",
        "        \"description\": \"Good for answering questions about physics\",\n",
        "        \"prompt_template\": physics_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template\n",
        "    }\n",
        "]\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "  name = p_info[\"name\"]\n",
        "  prompt_template = p_info[\"prompt_template\"]\n",
        "  prompt = PromptTemplate.from_template(prompt_template)\n",
        "  chain = LLMChain(llm=llm, prompt=prompt)\n",
        "  destination_chains[name] = chain\n",
        "\n",
        "default_chain = ConversationChain(llm=llm, output_key=\"text\")\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser()\n",
        ")\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
        "\n",
        "chain = MultiPromptChain(\n",
        "    router_chain=router_chain,\n",
        "    destination_chains=destination_chains,\n",
        "    default_chain=default_chain,\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "AOTk-nrHa6e8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.run(\"What is black body radiation?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yingw_Stdk2X",
        "outputId": "2c65df65-307f-4b22-fdbc-447ed1fa70b7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "physics: {'input': 'What is black body radiation?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "Black body radiation is the electromagnetic radiation emitted by a black body in thermodynamic equilibrium. It is a type of thermal radiation, and is the result of the thermal energy of the body's particles being converted into electromagnetic energy. The spectrum of the radiation is determined by the temperature of the body, and is a function of the temperature of the body.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.run(\"What is the name of the type of cloud that rins\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRBW09uIdw-C",
        "outputId": "4d1ea03d-be82-40df-cb90-63d672491767"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "None: {'input': 'What is the name of the type of cloud that rains?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " The type of cloud that rains is called a cumulonimbus cloud. It is a large, dense cloud that is usually associated with thunderstorms.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EmbeddingRouterChain\n",
        "\n",
        "The `EmbeddingRouterChain` uses embeddings and similarity to route between destination chains."
      ],
      "metadata": {
        "id": "ePOMNJi3d4oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.router.embedding_router import EmbeddingRouterChain\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "names_and_descriptions = [\n",
        "    (\"physics\", [\"for questions about physics\"]),\n",
        "    (\"math\", [\"for questions about math\"])\n",
        "]\n",
        "\n",
        "router_chain = EmbeddingRouterChain.from_names_and_descriptions(\n",
        "    names_and_descriptions,\n",
        "    FAISS,\n",
        "    OpenAIEmbeddings(),\n",
        "    routing_keys=[\"input\"]\n",
        ")\n",
        "\n",
        "chain = MultiPromptChain(\n",
        "    router_chain=router_chain,\n",
        "    destination_chains=destination_chains,\n",
        "    default_chain=default_chain,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "ir-eCymBd8IW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.run(\"What is black body radiation?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soPtCYV2eih7",
        "outputId": "a6b22092-bc19-4cf5-b28c-764987f073cc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "physics: {'input': 'What is black body radiation?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "Black body radiation is the electromagnetic radiation emitted by a black body in thermodynamic equilibrium. It is a type of thermal radiation, and is the result of the thermal energy of the body being converted into electromagnetic radiation. The spectrum of the radiation is determined by the temperature of the body, and is a function of the body's emissivity.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.run(\"What is the name of the type of cloud that rins\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZR53FwOeoCS",
        "outputId": "acd52758-a492-4164-9773-ea656404ecbb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "physics: {'input': 'What is the name of the type of cloud that rins'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "es rain?\n",
            "\n",
            "The type of cloud that produces rain is called a cumulonimbus cloud.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequential Chains\n",
        "\n",
        "The next step after calling a language model is make a series of calls to a language model. This is particularly useful when you want to take the output from one call and use it as the input to another.\n",
        "\n",
        "In this notebook we will walk through some examples for how to do this, using sequential chains. Sequential chains allow you to connect multiple chains and compose them into pipelines that execute some specific scenario.. There are two types of sequential chains:\n",
        "\n",
        "* `SimpleSequentialChain`: The simplest form of sequential chains, where each step has a singular input/output, and the output of one step is the input to the next.\n",
        "* `SequentialChain`: A more general form of sequential chains, allowing for multiple inputs/outputs."
      ],
      "metadata": {
        "id": "QNPKRx9-e63p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SimpleSequentialChain"
      ],
      "metadata": {
        "id": "AupwR4lfgIWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# This is an LLMChain to write a synopsis given a title of a play.\n",
        "llm = OpenAI(temperature=0.7)\n",
        "template = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\n",
        "\n",
        "Title: {title}\n",
        "Playwright: This is a synopsis for the above play:\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "synopsis_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# This is an LLMChain to write a review of a play given a synopsis.\n",
        "llm = OpenAI(temperature=.7)\n",
        "template = \"\"\"You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n",
        "\n",
        "Play Synopsis:\n",
        "{synopsis}\n",
        "Review from a New York Times play critic of the above play:\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "review_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# This is the overall chain where we run these two chains in sequence.\n",
        "sequential_chain = SimpleSequentialChain(chains=[synopsis_chain, review_chain], verbose=True)\n",
        "\n",
        "print(sequential_chain.run(\"Tragedy at sunset on the beach\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDHzUyKcfCh_",
        "outputId": "ca01279c-9200-461a-a9b3-7546ff6bf17c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m\n",
            "\n",
            "Tragedy at Sunset on the Beach is a story of a family's struggle to survive a difficult situation. The setting is a small beach town in the Pacific Northwest. The family consists of two parents, their daughter, and their son. The parents, both having lost their jobs, have been struggling to make ends meet. \n",
            "\n",
            "The daughter, being the oldest, takes on the role of provider in order to help the family. She works hard to pay the bills and to put food on the table. Her brother, meanwhile, is trying to find his way in life. He has dreams of becoming a successful musician, but his parents are not supportive of his ambitions. \n",
            "\n",
            "One day, the daughter meets a mysterious stranger on the beach. He asks her for a favor, and she agrees, unaware of what she has gotten herself into. As the sun sets, tragedy unfolds and the family's lives are changed forever. \n",
            "\n",
            "The play follows the family as they try to pick up the pieces and move on with their lives. Through tragedy, they discover the strength of their bond, the power of forgiveness, and the importance of living life to the fullest.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "\n",
            "Tragedy at Sunset on the Beach is a heart-wrenching story of a family's struggle to survive in a difficult situation. The story is set in a small beach town in the Pacific Northwest and follows the family of two parents, their daughter, and son. With the parents having both lost their jobs, the daughter takes on the role of provider and works hard to support the family while her brother seeks to fulfill his dreams of becoming a successful musician. One day, the daughter meets a mysterious stranger who asks her for a favor, and tragedy ensues.\n",
            "\n",
            "The play explores how the family deals with the tragedy and finds the strength to move on with their lives. Through the dark moments, the family discovers the power of forgiveness and the importance of living life to the fullest. The cast of this production brings out the depth of the characters and their emotions, with strong performances that will have you both laughing and crying. Director Julia Smith does an excellent job of showing the family's journey and the play's resolution is both satisfying and moving. \n",
            "\n",
            "Tragedy at Sunset on the Beach is a powerful story of family and resilience that will stay with you long after you leave the theater. Highly recommended.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "Tragedy at Sunset on the Beach is a heart-wrenching story of a family's struggle to survive in a difficult situation. The story is set in a small beach town in the Pacific Northwest and follows the family of two parents, their daughter, and son. With the parents having both lost their jobs, the daughter takes on the role of provider and works hard to support the family while her brother seeks to fulfill his dreams of becoming a successful musician. One day, the daughter meets a mysterious stranger who asks her for a favor, and tragedy ensues.\n",
            "\n",
            "The play explores how the family deals with the tragedy and finds the strength to move on with their lives. Through the dark moments, the family discovers the power of forgiveness and the importance of living life to the fullest. The cast of this production brings out the depth of the characters and their emotions, with strong performances that will have you both laughing and crying. Director Julia Smith does an excellent job of showing the family's journey and the play's resolution is both satisfying and moving. \n",
            "\n",
            "Tragedy at Sunset on the Beach is a powerful story of family and resilience that will stay with you long after you leave the theater. Highly recommended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequential Chain\n",
        "\n",
        "Of course, not all sequential chains will be as simple as passing a single string as an argument and getting a single string as output for all steps in the chain. In this next example, we will experiment with more complex chains that involve multiple inputs, and where there also multiple final outputs.\n",
        "\n",
        "Of particular importance is how we name the input/output variable names. In the above example we didn't have to think about that because we were just passing the output of one chain directly as input to the next, but here we do have worry about that because we have multiple inputs."
      ],
      "metadata": {
        "id": "Pp2SdVsKgLlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "# This is an LLMChain to write a synopsis given a title of a play and the era it is set in.\n",
        "llm = OpenAI(temperature=0.7)\n",
        "template = \"\"\"You are a playwright. Given the title of play and the era it is set in, it is your job to write a synopsis for that title.\n",
        "\n",
        "Title: {title}\n",
        "Era: {era}\n",
        "Playwright: This is a synopsis for the above play:\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "synopsis_chain = LLMChain(llm=llm, prompt=prompt, output_key=\"synopsis\")\n",
        "\n",
        "# This is an LLMChain to write a review of a play given a synopsis.\n",
        "llm = OpenAI(temperature=0.7)\n",
        "template = \"\"\"You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n",
        "\n",
        "Play Synopsis:\n",
        "{synopsis}\n",
        "Review from a New York Times play critic of the above play:\"\"\"\n",
        "prompt_template = PromptTemplate.from_template(template)\n",
        "review_chain = LLMChain(llm=llm, prompt=prompt, output_key=\"review\")\n",
        "\n",
        "# This is the overall chain where we run these two chains in sequence.\n",
        "sequential_chain = SequentialChain(\n",
        "    chains=[synopsis_chain, review_chain],\n",
        "    input_variables=[\"era\", \"title\"],\n",
        "    output_variables=[\"synopsis\", \"review\"],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "sequential_chain({\"title\":\"Tragedy at sunset on the beach\", \"era\": \"Victorian England\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6hHWtW1gXBJ",
        "outputId": "68ef511e-1c32-486b-e6a3-1dff8308700b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'Tragedy at sunset on the beach',\n",
              " 'era': 'Victorian England',\n",
              " 'synopsis': \"\\n\\nTragedy at sunset on the beach is a heart-wrenching tale of love and loss set in Victorian England. The play focuses on two star-crossed lovers, William and Susannah, who have been forbidden to be together by their families. Despite the odds, the two continue to meet clandestinely at the beach near their homes.\\n\\nOne fateful evening, as the sun sets over the beach, the two finally confess their true feelings for each other. But as they embrace, tragedy strikes as a rogue wave crashes upon the shore, sweeping Susannah out to sea. William dives in to save her, but it is too late and Susannah is lost forever.\\n\\nThe play follows William's journey in the aftermath of Susannah's death. In his grief, he is desperate to find some way to bring her back. He turns to a variety of characters, from a priest to a mad scientist, in his quest to reunite with his beloved. In the end, he finds a way to bring Susannah back to life, but at a terrible price.\\n\\nUltimately, Tragedy at Sunset on the Beach is a tale of love, loss, and the lengths we will go to for those we love. Through William's journey\",\n",
              " 'review': \"\\n\\nThe play follows the story of a young woman named Eliza who lives in Victorian England. Eliza is a beautiful and kind-hearted girl who is passionate about life and loves to spend her days exploring the beach near her home.\\n\\nOne fateful day, Eliza is walking along the beach near sunset and encounters a stranger who captures her heart. The man is mysterious and full of secrets, but Eliza is intrigued and quickly falls in love with him.\\n\\nThe couple's romance blossoms over the following weeks and Eliza finds herself daydreaming about the future they could have together. However, tragedy strikes when the man suddenly disappears without a trace. Eliza is left heartbroken and confused, unable to comprehend what has happened to her beloved.\\n\\nIn the aftermath of the tragedy, Eliza is left to pick up the pieces of her broken heart and face a future without her true love. Through her struggles, she learns to find strength in herself and the courage to keep going despite the pain of loss.\\n\\nThe play ends with Eliza standing on the beach at sunset, accepting her fate and looking towards a brighter future.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory in Sequential Chains\n",
        "\n",
        "Sometimes you may want to pass along some context to use in each step of the chain or in a later part of the chain, but maintaining and chaining together the input/output variables can quickly get messy. Using `SimpleMemory` is a convenient way to do manage this and clean up your chains.\n",
        "\n",
        "For example, using the previous playwright SequentialChain, lets say you wanted to include some context about date, time and location of the play, and using the generated synopsis and review, create some social media post text. You could add these new context variables as `input_variables`, or we can add a `SimpleMemory` to the chain to manage this context:"
      ],
      "metadata": {
        "id": "nv9XuYzjiBvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "from langchain.memory import SimpleMemory\n",
        "\n",
        "llm = OpenAI(temperature=.7)\n",
        "template = \"\"\"You are a social media manager for a theater company.  Given the title of play, the era it is set in, the date,time and location, the synopsis of the play, and the review of the play, it is your job to write a social media post for that play.\n",
        "\n",
        "Here is some context about the time and location of the play:\n",
        "Date and Time: {time}\n",
        "Location: {location}\n",
        "\n",
        "Play Synopsis:\n",
        "{synopsis}\n",
        "Review from a New York Times play critic of the above play:\n",
        "{review}\n",
        "\n",
        "Social Media Post:\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "social_chain = LLMChain(llm=llm, prompt=prompt, output_key=\"social_post_text\")\n",
        "\n",
        "sequential_chain = SequentialChain(\n",
        "    memory=SimpleMemory(memories={\"time\": \"December 25th, 8pm PST\", \"location\": \"Theater in the Park\"}),\n",
        "    chains=[synopsis_chain, review_chain, social_chain],\n",
        "    input_variables=[\"era\", \"title\"],\n",
        "    output_variables=[\"social_post_text\"],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "sequential_chain({\"title\":\"Tragedy at sunset on the beach\", \"era\": \"Victorian England\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5UsTIBPiJfZ",
        "outputId": "996daed9-fa56-426f-f56d-de8252e2c4ee"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'Tragedy at sunset on the beach',\n",
              " 'era': 'Victorian England',\n",
              " 'time': 'December 25th, 8pm PST',\n",
              " 'location': 'Theater in the Park',\n",
              " 'social_post_text': \"\\nWe invite you to experience a heart-wrenching tale of love, tragedy, and oppression this December 25th at 8pm PST at Theater in the Park. Tragedy at Sunset on the Beach is set in Victorian England and follows the story of three women - a young maid, a matronly widow, and a socialite - as they come together on the beach at sunset. Experience the grief and sorrow of these characters as they face the forces of society that shape their lives. Don't miss this powerful and captivating story, brought to you by our amazing actors and crew! Get your tickets today! #TragedyAtSunset #LoveAndLoss #VictorianEngland\"}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}