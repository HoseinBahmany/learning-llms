{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTphF5M7VdFJXYygqhHF3q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoseinBahmany/learning-llms/blob/main/spacy/getting_started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VIg0AYBGfrl"
      },
      "outputs": [],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "TU8HFcgsHa5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "doc = nlp(\"It's been a crazy week!!!\")\n",
        "print([token.text for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaYfuVjvHna1",
        "outputId": "3a8a131f-9025-43eb-c2d1-69f4a8faa1e9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', \"'s\", 'been', 'a', 'crazy', 'week', '!', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customizing the tokenizer\n",
        "\n",
        "When we work with a specific domain such as medicine, insurance, or finance, we often come across words, abbreviations, and entities that needs special attention. Most domains that you'll process have characteristic words and phrases that need custom tokenization rules. Here's how to add a special case rule to an existing Tokenizer class instance:\n",
        "\n"
      ],
      "metadata": {
        "id": "8uIXPXJjfTHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.symbols import ORTH\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "doc = nlp(\"lemme that\")\n",
        "print([d.text for d in doc])\n",
        "\n",
        "special_case = [{ORTH: \"lem\"}, {ORTH: \"me\"}]\n",
        "nlp.tokenizer.add_special_case(\"lemme\", special_case)\n",
        "doc = nlp(\"lemme that\")\n",
        "print([d.text for d in doc])\n",
        "\n",
        "# If you define a special case rule with punctuation, the special case rule will take precedence over the punctuation splitting:\n",
        "nlp.tokenizer.add_special_case(\"...lemme...?\", [{\"ORTH\": \"...lemme...?\"}])\n",
        "print([w.text for w in nlp(\"...lemme...?\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzvG4BvvfXPS",
        "outputId": "6d25f5da-0aab-41af-da65-93df20de684d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lemme', 'that']\n",
            "['lem', 'me', 'that']\n",
            "['...lemme...?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence segmentation\n",
        "\n",
        "We saw that breaking a sentence into its tokens is not a straightforward task at all. How about breaking a text into sentences? It's indeed a bit more complicated to mark where a sentence starts and ends due to the same reasons of punctuation, abbreviations, and so on.\n",
        "\n",
        "A Doc object's sentences are available via the doc.sents property:"
      ],
      "metadata": {
        "id": "e-56YgUbh_0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "doc = nlp(\"I flied to N.Y yesterday. It was around 5 pm.\")\n",
        "print([sent.text for sent in doc.sents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dMy9NnBgr8y",
        "outputId": "4e23d1b7-f8ce-4f39-9a30-8a0fd2c2edb6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I flied to N.Y yesterday.', 'It was around 5 pm.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determining sentence boundaries is a more complicated task than tokenization. As a result, spaCy uses the dependency parser to perform sentence segmentation."
      ],
      "metadata": {
        "id": "HVdRPU--igOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding lemmatization\n",
        "\n",
        "A lemma is the base form of a token. You can think of a lemma as the form in which the token appears in a dictionary. For instance, the lemma of eating is eat; the lemma of eats is eat; ate similarly maps to eat. Lemmatization is the process of reducing the word forms to their lemmas. The following code is a quick example of how to do lemmatization with spaCy:"
      ],
      "metadata": {
        "id": "LvL81Pluinny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "doc = nlp(\"I went there for working and worked for 3 years.\")\n",
        "for token in doc:\n",
        "  print(token.text, token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMYVcgktivOz",
        "outputId": "0958827e-04c2-4717-fc4d-7a9b54ca017b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I I\n",
            "went go\n",
            "there there\n",
            "for for\n",
            "working work\n",
            "and and\n",
            "worked work\n",
            "for for\n",
            "3 3\n",
            "years year\n",
            ". .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization in NLU\n",
        "\n",
        "Suppose that you design an NLP pipeline for a ticket booking system. Your application processes a customer's sentence, extracts necessary information from it, and then passes it to the booking API.\n",
        "\n",
        "The NLP pipeline wants to extract the form of the travel (a flight, bus, or train), the destination city, and the date. The first thing the application needs to verify is the means of travel:\n",
        "\n",
        "```\n",
        "fly – flight – airway – airplane - plane\n",
        "\n",
        "bus\n",
        "\n",
        "railway – train\n",
        "```\n",
        "\n",
        "We have this list of keywords and we want to recognize the means of travel by searching the tokens in the keywords list. The most compact way of doing this search is by looking up the token's lemma. Consider the following customer sentences:\n",
        "\n",
        "```\n",
        "List me all flights to Atlanta.\n",
        "\n",
        "I need a flight to NY.\n",
        "\n",
        "I flew to Atlanta yesterday evening and forgot my baggage.\n",
        "```"
      ],
      "metadata": {
        "id": "Qgy3MmUJsPMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.attrs import ORTH, NORM\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "# special_case = [{ORTH: 'Angeltown', NORM: 'Los Angeles'}]\n",
        "# nlp.tokenizer.add_special_case(\"Angeltown\", special_case)\n",
        "\n",
        "doc = nlp(\"I'm flying to Angeltown\")\n",
        "\n",
        "for token in doc:\n",
        "  print(token.text, token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaIS1ZULs3G5",
        "outputId": "8d4e28ed-f955-41ba-ba45-4b22be30d463"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I I\n",
            "'m be\n",
            "flying fly\n",
            "to to\n",
            "Angeltown Angeltown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the difference between lemmatization and stemming\n",
        "\n",
        "A lemma is the base form of a word and is always a member of the language's vocabulary. The stem does not have to be a valid word at all. For instance, the lemma of improvement is improvement, but the stem is improv. You can think of the stem as the smallest part of the word that carries the meaning.\n",
        "\n",
        "Both stemming and lemmatization have their own advantages. Stemming gives very good results if you apply only statistical algorithms to the text, without further semantic processing such as pattern lookup, entity extraction, coreference resolution, and so on. Also stemming can trim a big corpus to a more moderate size and give you a compact representation. If you also use linguistic features in your pipeline or make a keyword search, include lemmatization. Lemmatization algorithms are accurate but come with a cost in terms of computation."
      ],
      "metadata": {
        "id": "u3iX-Qrpvtac"
      }
    }
  ]
}